<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/xxw/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cedricchen.xyz","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CUDA-C是一种用于通用计算的并行编程模型，专门为NVIDIA的GPU架构设计。随着数据量和计算需求的不断增长，传统的CPU在处理某些计算密集型任务（如图像处理、科学计算、深度学习）时可能会变得非常缓慢。GPU的并行处理能力可以显著提高这些任务的执行速度。如矩阵运算、信号处理和物理模拟等，通过CUDA-C编程，可以将这些任务移植到GPU上运行，极大提升计算效率。本文从四个方面进行CUDA介绍，第">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA编程基础">
<meta property="og:url" content="https://cedricchen.xyz/2024/09/02/CUDA-C%E7%BC%96%E7%A8%8B/">
<meta property="og:site_name" content="shushu学通信">
<meta property="og:description" content="CUDA-C是一种用于通用计算的并行编程模型，专门为NVIDIA的GPU架构设计。随着数据量和计算需求的不断增长，传统的CPU在处理某些计算密集型任务（如图像处理、科学计算、深度学习）时可能会变得非常缓慢。GPU的并行处理能力可以显著提高这些任务的执行速度。如矩阵运算、信号处理和物理模拟等，通过CUDA-C编程，可以将这些任务移植到GPU上运行，极大提升计算效率。本文从四个方面进行CUDA介绍，第">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240726162157185.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240729150959100.png">
<meta property="og:image" content="https://cedricchen.xyz/images/CPU+GPU.png">
<meta property="og:image" content="https://cedricchen.xyz/images/cudanew.png">
<meta property="og:image" content="https://face2ai.com/CUDA-F-2-1-CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B02/1.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240808155953825.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240809152101683.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240810175913252.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240828153147100.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240906151243038.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240910091357014.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240910091857453.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240925134203201.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240920161956458.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240920162015488.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240920162227886.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240920162547354.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240921162536356.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240921163332876.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240921163623545.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240923162806399.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240923163215192.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240923163336844.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240923163540081.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240923163745596.png">
<meta property="og:image" content="https://img.androsheep.win/blog/CWA_example.png">
<meta property="og:image" content="https://cedricchen.xyz/images/Occupancy_example.png">
<meta property="og:image" content="https://cedricchen.xyz/images/image-20240923193742385.png">
<meta property="article:published_time" content="2024-09-01T16:00:00.000Z">
<meta property="article:modified_time" content="2024-10-26T12:15:47.072Z">
<meta property="article:author" content="Cedric Chen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cedricchen.xyz/images/image-20240726162157185.png">

<link rel="canonical" href="https://cedricchen.xyz/2024/09/02/CUDA-C%E7%BC%96%E7%A8%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>CUDA编程基础 | shushu学通信</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


</head>


<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">shushu学通信</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ArdrewChen" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://cedricchen.xyz/2024/09/02/CUDA-C%E7%BC%96%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="Cedric Chen">
      <meta itemprop="description" content="对着生活哈哈大笑">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="shushu学通信">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CUDA编程基础
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-02 00:00:00" itemprop="dateCreated datePublished" datetime="2024-09-02T00:00:00+08:00">2024-09-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-26 20:15:47" itemprop="dateModified" datetime="2024-10-26T20:15:47+08:00">2024-10-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">知识</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9F%A5%E8%AF%86/GPU/" itemprop="url" rel="index"><span itemprop="name">GPU</span></a>
                </span>
            </span>

          
            <div class="post-description">CUDA-C是一种用于通用计算的并行编程模型，专门为NVIDIA的GPU架构设计。随着数据量和计算需求的不断增长，传统的CPU在处理某些计算密集型任务（如图像处理、科学计算、深度学习）时可能会变得非常缓慢。GPU的并行处理能力可以显著提高这些任务的执行速度。如矩阵运算、信号处理和物理模拟等，通过CUDA-C编程，可以将这些任务移植到GPU上运行，极大提升计算效率。本文从四个方面进行CUDA介绍，第一部分介绍GPU的内部的硬件组成。第二部分基于谭升大佬的博客，具体CUDA编程的实现和一些并行优化思想，第二部分包含基于Windows平台下的一些代码实现。第三部分介绍CUDA开发常用的一些函数，主要是为了方便查找。工先善其事必先利其器，第四部分讲述CUDA调试的一些工具的使用方法。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>距离上次上传文章已经过了快半年了，想想这半年或许有些懈怠，不过这半年确实有些浑浑噩噩，无论是实验室的项目、论文，还是一些生活上的小事，都过于劳费心神了，当初博客的规划也完成的七零八落的，当时计划开一个读书感想，想想这半年读的书也少了，想想以前的东西，倒有一种提笔忘字的感觉。也不排除当时是刚买来域名，图一个新鲜感。安静思考下来，倒是觉得浪费青春了，想得太多做的太少，又常常羡慕一些大佬的才华，却又少了一些踏实的行动，实非不可举。</p>
<p>跳过前面的煽情的部分，这篇文章的第二部分及其之后的内容尚未更新结束，第四部分已经出现在计划表半个月了，至今还是寥寥数笔，还是太懈怠了，希望发完之后能起个督促作用吧。再次感谢<a target="_blank" rel="noopener" href="https://face2ai.com/program-blog/#GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89">谭升大佬的博客</a>，对于一个初学者来说，这篇博客解答了很多我的疑惑，也成功带我开始走进CUDA的世界。</p>
<h1 id="CUDA硬件结构"><a href="#CUDA硬件结构" class="headerlink" title="CUDA硬件结构"></a>CUDA硬件结构</h1><h2 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h2><p><img src="/../images/image-20240726162157185.png" alt="image-20240726162157185"></p>
<h2 id="各部分具体结构"><a href="#各部分具体结构" class="headerlink" title="各部分具体结构"></a>各部分具体结构</h2><h3 id="HBM"><a href="#HBM" class="headerlink" title="HBM"></a>HBM</h3><p>HBM全称为High Bandwidth Memory，即高带宽内存，是一款新型的CPU&#x2F;GPU内存芯片。 其实就是将很多个DDR芯片堆叠在一起后和GPU封装在一起，实现大容量、高位宽的DDR组合阵列。</p>
<h3 id="Memory-Cotroller"><a href="#Memory-Cotroller" class="headerlink" title="Memory Cotroller"></a>Memory Cotroller</h3><p>负责控制图形卡的内存访问。</p>
<h3 id="GPC、TPC和SM"><a href="#GPC、TPC和SM" class="headerlink" title="GPC、TPC和SM"></a>GPC、TPC和SM</h3><p>GPU包含若干GPC (Graphics Processing Cluster, 图形处理簇)组成的阵列。GPC又包含若干TPC (Texture Processing Cluster)。TPC中包含若干SM (Stream Multiprocessor,流多处理器）。SM中包含若干CUDA Core和Tensor Core。</p>
<h3 id="L2-Cache"><a href="#L2-Cache" class="headerlink" title="L2 Cache"></a>L2 Cache</h3><p>所有的 SM 共享 L2 缓存。可以通过缓存访问全局内存中的数据。</p>
<h3 id="NVLink"><a href="#NVLink" class="headerlink" title="NVLink"></a>NVLink</h3><p>NVLink 是一种GPU 之间的直接互连，双向互连速度达1.8 TB&#x2F;s，可扩展服务器内的多GPU 输入&#x2F;输出(IO)。实现多GPU通信。</p>
<h3 id="High-Speed-Hub"><a href="#High-Speed-Hub" class="headerlink" title="High-Speed Hub"></a>High-Speed Hub</h3><p>在GPU架构接口层面上，NVLink控制器通过另一个名为High-Speed Hub（HSHUB）的新块与GPU内部通信。HSHUB直接访问GPU宽交叉开关和其他系统元素，例如高速复制引擎（HSCE），可用于以最高NVLink速率将数据移动进入和移出GPU。</p>
<h3 id="GigaThread-Engine-MIG-Control"><a href="#GigaThread-Engine-MIG-Control" class="headerlink" title="GigaThread Engine MIG Control"></a>GigaThread Engine MIG Control</h3><p>由PCIe接口进入的计算任务，通过带有多实例GPU（Multi-Instance GPU，MIG）控制的GigaThread引擎分配给各个GPC。GPC之间通过L2缓存共享中间数据，GPC计算的中间数据通过NVLink与其他GPU连接&#x2F;交换。每个TPC由2个流式多处理器（Streaming Multiprocessor，SM）组成。</p>
<h2 id="GPU的内部存储"><a href="#GPU的内部存储" class="headerlink" title="GPU的内部存储"></a>GPU的内部存储</h2><p>GPU 内存可以分为：局部内存（local memory）、全局内存（global memory）、常量内存（constant memory）、共享内存（shared memory）、寄存器（register）、L1&#x2F;L2 缓存等。其中全局内存、局部内存、常量内存都是片下内存，<strong>储存在 HBM 上</strong>。</p>
<h3 id="全局内存"><a href="#全局内存" class="headerlink" title="全局内存"></a>全局内存</h3><p>全局内存（global memory）能被 GPU 的所有线程访问，全局共享。它是片下（off chip）内存。跟 CPU 架构一样，运算单元不能直接使用全局内存的数据，需要经过缓存。</p>
<h3 id="L1-L2缓存"><a href="#L1-L2缓存" class="headerlink" title="L1&#x2F;L2缓存"></a>L1&#x2F;L2缓存</h3><p>L2 缓存可以被所有 SM 访问，速度比全局内存快；L1 缓存用于存储 SM 内的数据，被 SM 内的 CUDA cores 共享，但是跨 SM 之间的 L1 不能相互访问。</p>
<p>合理运用 L2 缓存能够提速运算。A100 的 L2 缓存能够设置至多 40MB 的持续化数据 (persistent data)，能够拉升算子 kernel 的带宽和性能。Flash attention 的思路就是<strong>尽可能地利用 L2 缓存，减少 HBM 的数据读写时间。</strong></p>
<h3 id="局部内存"><a href="#局部内存" class="headerlink" title="局部内存"></a>局部内存</h3><p>局部内存 (local memory) 是<strong>线程独享</strong>的内存资源，线程之间不可以相互访问。局部内存属于片下内存，所以访问速度跟全局内存一样。它主要是用来应对寄存器不足时的场景，即在线程申请的变量超过可用的寄存器大小时，nvcc 会自动将一部数据放置到片下内存里。</p>
<h3 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h3><p>寄存器（register）是<strong>线程能独立访问</strong>的资源，它是片上（on chip）存储，用来存储一些线程的暂存数据。寄存器的速度是访问中最快的，但是它的容量较小，只有几百甚至几十 KB，而且要被许多线程均分。</p>
<h3 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h3><p>共享内存（shared memory) 是一种在线程块内能访问的内存，是片上（on chip）存储，访问速度较快。</p>
<p>共享内存主要是缓存一些需要反复读写的数据。</p>
<p>注：共享内存与 L1 缓存的位置、速度极其类似，区别在于共享内存的控制与生命周期管理与 L1 不同：<strong>共享内存受用户控制，L1 受系统控制</strong>。共享内存更利于线程块之间数据交互。</p>
<h3 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h3><p>常量内存（constant memory）是片下（off chip）存储，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，它是只读内存。</p>
<p>常量内存主要是解决一个 warp scheduler 内多个线程访问相同数据时速度太慢的问题。假设所有线程都需要访问一个 constant_A 的常量，在存储介质constant_A 的数据只保存了一份，而内存的物理读取方式<strong>决定了多个线程不能在同一时刻读取到该变量，所以会出现先后访问的问题</strong>，这样使得并行计算的线程出现了运算时差。常量内存正是解决这样的问题而设置的，它有对应的 cache 位置产生多个副本，让线程访问时不存在冲突，从而保证并行度。</p>
<h2 id="流式处理器（SM）"><a href="#流式处理器（SM）" class="headerlink" title="流式处理器（SM）"></a>流式处理器（SM）</h2><p>进入SM单元的指令首先存入L1指令缓存（L1 Instruction Cache），然后再分发到L0指令缓存（L1 Instruction Cache）。与L0缓存配套的<strong>线程束排序器</strong>（<strong>Wrap Scheduler）</strong>和<strong>调度单元（Dispatch Unit）</strong>来为CUDA核心和张量核心分配计算任务。（注：GPU中最小的硬件计算执行单位是线程束，简称Warp。）</p>
<p><img src="/../images/image-20240729150959100.png" alt="SM内部结构"></p>
<h3 id="Warp-Scheduler-线程调度器"><a href="#Warp-Scheduler-线程调度器" class="headerlink" title="Warp Scheduler(线程调度器)"></a>Warp Scheduler(线程调度器)</h3><p>warp是GPU中<strong>最小的执行单元</strong>，由一组并行执行的线程组成，通常是32个线程。</p>
<p>Warp Scheduler负责选择哪个warp在下一个时钟周期内执行。它根据warp的状态（如是否有待执行的指令、是否有数据依赖等）来决定选择哪个warp。</p>
<p>每个SM通常配备多个warp调度器（如Volta架构中每个SM有4个warp调度器）。这些调度器能够并行调度多个warp，从而提高指令级并行性和吞吐量。</p>
<h3 id="Dispatch-Unit-调度单元"><a href="#Dispatch-Unit-调度单元" class="headerlink" title="Dispatch Unit(调度单元)"></a>Dispatch Unit(调度单元)</h3><p>Dispatch Unit从指令缓存中获取指令，并将这些指令分发给适当的执行单元（如整数运算单元、浮点运算单元、特殊功能单元等）。</p>
<p>Dispatch Unit负责从Warp Scheduler处接收准备好执行的warp，并将这些warp的指令分发给执行单元。Warp Scheduler管理多个warp的状态和调度，而Dispatch Unit<strong>具体执行这些warp的指令分发</strong>。</p>
<h3 id="LD-ST-存储队列"><a href="#LD-ST-存储队列" class="headerlink" title="LD&#x2F;ST(存储队列)"></a>LD&#x2F;ST(存储队列)</h3><p>处理从各种内存层次（包括全局内存、共享内存和本地内存）进行的数据传输操作。</p>
<h3 id="SFU-特殊计算单元"><a href="#SFU-特殊计算单元" class="headerlink" title="SFU(特殊计算单元)"></a>SFU(特殊计算单元)</h3><p>用于运算超越函数（sin、cos、exp、log……）这是因为 3D 游戏中所有的立体形状其实都是由微小的三角形拼接而来，而显卡要计算的就是这些三角形的平移、旋转等等。</p>
<h3 id="L1数据缓存"><a href="#L1数据缓存" class="headerlink" title="L1数据缓存"></a>L1数据缓存</h3><p>L1数据缓存主要用于缓存从全局内存中加载的数据，以加快数据访问速度。</p>
<p>它是硬件自动管理的缓存，程序员不需要显式地管理或控制。</p>
<h3 id="Tex"><a href="#Tex" class="headerlink" title="Tex"></a>Tex</h3><p>在NVIDIA GPU的Streaming Multiprocessor (SM) 中，Tex单元（Texture Units）是负责处理纹理内存访问的关键组件。纹理单元用于加速特定类型的内存访问模式，特别是在计算机图形学和一些科学计算应用中非常有用。它们提供了高效的内存访问路径，并支持一些特殊功能，如纹理过滤和地址计算。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>每个 SM 包含 4 个 processing blocks，它们共用这个 SM 的 <strong>L1 Instruction Cache</strong>（一级指令缓存）、<strong>L1 Data Cache</strong>（一级数据缓存）、<strong>Tex</strong>（纹理缓存，Texture cache）</p>
<p>把大量这样的 SM 排布在一起，将它们连接在 L2 Cache 和全局的调度器（GigaThread Engine）上，再为整张芯片设置与外部通信的线路——这就是用于 Data Center 的安培架构显示核心 <strong>GA100</strong> 的所有组成成分。</p>
<h1 id="CUDA软件编程"><a href="#CUDA软件编程" class="headerlink" title="CUDA软件编程"></a>CUDA软件编程</h1><h2 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h2><h3 id="异构"><a href="#异构" class="headerlink" title="异构"></a>异构</h3><p>不同的计算机架构就是异构</p>
<h3 id="CPU-GPU异构架构"><a href="#CPU-GPU异构架构" class="headerlink" title="CPU+GPU异构架构"></a>CPU+GPU异构架构</h3><p><img src="/../images/CPU+GPU.png"></p>
<p>CPU负责控制，GPU负责计算</p>
<h3 id="GPU计算指标"><a href="#GPU计算指标" class="headerlink" title="GPU计算指标"></a>GPU计算指标</h3><h4 id="容量特征"><a href="#容量特征" class="headerlink" title="容量特征"></a>容量特征</h4><ul>
<li><p>CUDA核心数量（越多越好）</p>
</li>
<li><p>显存大小（越大越好）</p>
</li>
</ul>
<h4 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h4><ul>
<li><p>峰值计算能力:代表GPU的最大计算能力</p>
</li>
<li><p>显存带宽：显存与数据单元的通信速率</p>
</li>
</ul>
<h3 id="CPU和GPU线程区别"><a href="#CPU和GPU线程区别" class="headerlink" title="CPU和GPU线程区别"></a>CPU和GPU线程区别</h3><ol>
<li>CPU线程是重量级实体，操作系统交替执行线程，线程上下文切换花销很大</li>
<li>GPU线程是轻量级的，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。</li>
<li>CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量</li>
</ol>
<h2 id="CUDA基本介绍"><a href="#CUDA基本介绍" class="headerlink" title="CUDA基本介绍"></a>CUDA基本介绍</h2><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p>CUDA nvcc编译器会自动分离代码里面的不同部分，如主机代码用C写成，使用本地的C语言编译器编译，设备端代码，也就是核函数，用CUDA C编写，通过nvcc编译，链接阶段，在内核程序调用或者明显的GPU设备操作时，添加运行时库。</p>
<h3 id="CUDA的API接口"><a href="#CUDA的API接口" class="headerlink" title="CUDA的API接口"></a>CUDA的API接口</h3><ul>
<li>CUDA驱动（driver）时API，相当于汇编语言，更加底层。</li>
<li>和CUDA驱动时（runtime）API</li>
<li>两者性能几乎无差异</li>
</ul>
<h3 id="编写程序流程"><a href="#编写程序流程" class="headerlink" title="编写程序流程"></a>编写程序流程</h3><ol>
<li>分配GPU内存</li>
<li>拷贝内存到设备</li>
<li>调用CUDA内核函数来执行计算</li>
<li>把计算完成数据拷贝回主机端</li>
<li>内存销毁</li>
</ol>
<h3 id="VS-CUDA环境配置"><a href="#VS-CUDA环境配置" class="headerlink" title="VS CUDA环境配置"></a>VS CUDA环境配置</h3><p>cuda安装完成之后，打开VS，新建项目，选择CUDA xx.xx runtime。</p>
<p><img src="/../images/cudanew.png"></p>
<p><strong>把.cu格式添加到编辑器和扩展名</strong></p>
<p>（工具–&gt;选项–&gt;文本编辑器–&gt;文件拓展名, 新增扩展名 .cu 并将编辑器设置为：Microsoft Visual C++。）</p>
<p>工具–&gt;选项–&gt;项目和解决方案–&gt;VC++项目设置，添加要包括的扩展名”.cu”.）</p>
<h3 id="示例——hello-world"><a href="#示例——hello-world" class="headerlink" title="示例——hello_world"></a>示例——hello_world</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 核函数hello_world</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">hello_world</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Hello World from GPU!\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 核函数过渡函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">kernel_hello_world</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	hello_world &lt;&lt; &lt;<span class="number">1</span>, <span class="number">5</span>&gt;&gt; &gt; ();</span><br><span class="line">	<span class="built_in">cudaDeviceReset</span>(); <span class="comment">//这句话如果没有，则不能正常的运行</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>注：</strong><code>cudaDeviceReset(); </code>这句话如果没有，则不能正常的运行，因为这句话包含了隐式同步，GPU和CPU执行程序是异步的，核函数调用后成立刻会到主机线程继续，而不管GPU端核函数是否执行完毕，所以上面的程序就是GPU刚开始执行，CPU已经退出程序了，所以我们要等GPU执行完了，再退出主机线程。</p>
<h2 id="CUDA编程模型"><a href="#CUDA编程模型" class="headerlink" title="CUDA编程模型"></a>CUDA编程模型</h2><p><strong>CUDA的编程主要涉及到对GPU内存和线程的控制</strong>。</p>
<h3 id="CUDA编程中的前缀"><a href="#CUDA编程中的前缀" class="headerlink" title="CUDA编程中的前缀"></a>CUDA编程中的前缀</h3><p><code>__host__ int foo(int a)&#123;&#125;</code>与C或者C++中的foo(int a){}相同，是由CPU调用，由CPU执行的函数<br><code>__global__ int foo(int a)&#123;&#125;</code>表示一个内核函数，是一组由GPU执行的并行计算任务，以foo&lt;&lt;&gt;&gt;(a)的形式或者driver API的形式调用。目前__global__函数必须由CPU调用，并将并行计算任务发射到GPU的任务调用单元。随着GPU可编程能力的进一步提高，未来可能可以由GPU调用。<br><code>__device__ int foo(int a)&#123;&#125;</code>则表示一个由GPU中一个线程调用的函数。由于Tesla架构的GPU允许线程调用函数，因此实际上是将__device__ 函数以__inline形式展开后直接编译到二进制代码中实现的，并不是真正的函数。</p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><h4 id="cuda内存管理API"><a href="#cuda内存管理API" class="headerlink" title="cuda内存管理API"></a>cuda内存管理API</h4><table>
<thead>
<tr>
<th align="center">标准C函数</th>
<th align="center">CUDA C 函数</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">malloc</td>
<td align="center">cudaMalloc</td>
<td align="center">内存分配</td>
</tr>
<tr>
<td align="center">memcpy</td>
<td align="center">cudaMemcpy</td>
<td align="center">内存复制</td>
</tr>
<tr>
<td align="center">memset</td>
<td align="center">cudaMemset</td>
<td align="center">内存设置</td>
</tr>
<tr>
<td align="center">free</td>
<td align="center">cudaFree</td>
<td align="center">释放内存</td>
</tr>
</tbody></table>
<h3 id="线程管理"><a href="#线程管理" class="headerlink" title="线程管理"></a>线程管理</h3><p>一个核函数<strong>只能有一个grid（网格）</strong>，一个grid可以有很多block(块)，一个块可以有很多thread(线程)。<strong>不同块内的线程是不能相互影响的</strong>，<strong>一个块内的线程是同步且共享内存的</strong></p>
<p>在内核函数调用：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_func&lt;&lt;&lt;M,N&gt;&gt;&gt;;</span><br></pre></td></tr></table></figure>

<p>这里的<code>kernel_func&lt;&lt;&lt;M,N&gt;&gt;&gt;</code>表示语法调用了M个线程块（block），一个线程块中包含N个线程。这里的<code>M</code>和<code>N</code>除了整型变量，也可以是<code>dim3</code>变量，指定一个<code>grid</code>中的<code>block</code>数量和一个<code>block</code>中<code>thread</code>的数量。</p>
<h4 id="线程标记"><a href="#线程标记" class="headerlink" title="线程标记"></a>线程标记</h4><p>为了让线程彼此区分开，因此需要使用标记区分线程。注意区分两个概念：<strong>线程ID是独一无二的</strong>，线程索引指的是一个块内的线程的索引值，<strong>不同块内的索引值可能一样。</strong></p>
<p>依靠下面两个基于uint3定义的内置结构体确定线程标号：</p>
<ul>
<li>blockIdx（线程块在线程网格内的位置索引）</li>
<li>threadIdx（线程在线程块内的位置索引）</li>
</ul>
<p><strong>注：</strong>uint3是cuda的一个内置变量类型,继承自基本整形和浮点型,为<strong>结构体</strong>,包含3个成员x,y和z。其中u表示无符号数。**而且这里的Idx表示<code>index</code>的缩写，不是<code>index x</code>**。</p>
<p>使用：</p>
<ul>
<li>blockIdx.x</li>
<li>blockIdx.y</li>
<li>blockIdx.z</li>
<li>threadIdx.x</li>
<li>threadIdx.y</li>
<li>threadIdx.z</li>
</ul>
<p>我们要有同样对应的两个结构体来保存其范围，也就是blockIdx中三个字段的范围threadIdx中三个字段的范围：</p>
<ul>
<li>blockDim</li>
<li>gridDim</li>
</ul>
<p><strong>注：</strong>在host，可以使用dim3定义grid和block的尺寸，作为kernel调用的一部分。dim3数据类型的手动定义的grid和block变量<strong>仅在host端可见</strong>。dim3是基于uint3的整数矢量类型。且未指定的组件都将初始化为。<code>dim thread(3,4)</code>，创建了一个二位的3<em>4的dim3变量。在<strong>设备端</strong>访问grid和block属性的数据类型是<strong>uint3不能修改的常类型结构</strong>。*<em>uint3是设备端在执行的时候可见的，不可以在核函数运行时修改，初始化完成后uint3值就不变了。</em></em></p>
<p>其使用过程如下：先用<code>dim3</code>类型指定<code>grid</code>和<code>block</code>，在核函数调用时，将<code>kernel_func&lt;&lt;&lt;grid block&gt;&gt;&gt;</code>，指定核函数调用时线程中一个网格中包括grid个块，一个块包含block个线程。线程布局如下图所示。</p>
<p><img src="https://face2ai.com/CUDA-F-2-1-CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B02/1.png" alt="img"></p>
<p><strong>举个例子：</strong>假设你有一个网格，其中包含多个块，每个块包含多个线程。例如，如果你的<code>blockDim</code>为(8, 8, 1)，这意味着每个块有64个线程，分布在8x8的网格中。如果你的<code>blockIdx</code>是(2, 3, 0)，这意味着你正在引用网格中第三行第四列的块（索引从0开始）。最后，如果<code>threadIdx</code>是(4, 5, 0)，则表示你正在指向块内第五行第六列的线程。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>; <span class="comment">// 定义一个block的dim3对象，并将block的x维度设为3，y和z的维度默认为1.</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong><code>blockDim</code>和<code>gridDim</code>只能在核函数中使用，在其余地方无法链接。</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>当主机启动了核函数，控制权马上回到主机，而不是主机等待设备完成核函数的运行。</p>
<p>如果核函数启动后的下一条指令就是从设备复制数据回主机端，那么主机端必须要等待设备端计算完成。</p>
<p>想要主机等待设备端执行可以用下面这个指令</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceSynchronize</span><span class="params">(<span class="type">void</span>)</span></span>;</span><br></pre></td></tr></table></figure>

<p>核函数都是异步执行的</p>
<h4 id="核函数编写限制"><a href="#核函数编写限制" class="headerlink" title="核函数编写限制"></a>核函数编写限制</h4><ol>
<li>只能访问设备内存</li>
<li>必须有void返回类型</li>
<li>不支持可变数量的参数</li>
<li>不支持静态变量</li>
<li>显示异步行为</li>
</ol>
<h4 id="核函数开发流程"><a href="#核函数开发流程" class="headerlink" title="核函数开发流程"></a>核函数开发流程</h4><p>核函数一般将串行的程序变为并行的程序，首先编写好串行程序和并行程序（核函数），然后验证核函数，即分别执行核函数和串行函数，然后调用以下程序</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">checkResult</span>(res_cpu,res_gpu,nthread)</span><br></pre></td></tr></table></figure>

<p>CUDA小技巧，当我们进行调试的时候可以把核函数配置成单线程的</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(argument list)</span><br></pre></td></tr></table></figure>

<h3 id="核函数计时"><a href="#核函数计时" class="headerlink" title="核函数计时"></a>核函数计时</h3><h4 id="CPU计时"><a href="#CPU计时" class="headerlink" title="CPU计时"></a>CPU计时</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">clock_t</span> start, finish;</span><br><span class="line">start = <span class="built_in">clock</span>();</span><br><span class="line"><span class="comment">// 要测试的部分</span></span><br><span class="line">finish = <span class="built_in">clock</span>();</span><br><span class="line">duration = (<span class="type">double</span>)(finish - start) / CLOCKS_PER_SEC;</span><br></pre></td></tr></table></figure>

<p>以上计时往往不准确，当需要获取准确的CPU计时，可参考如下程序</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Windows下CPU精准计时方法</span></span><br><span class="line">LARGE_INTEGER  large_interger;</span><br><span class="line"><span class="type">double</span> dff;</span><br><span class="line">__int64  c1, c2;</span><br><span class="line"><span class="type">double</span> duration;</span><br><span class="line"><span class="built_in">QueryPerformanceFrequency</span>(&amp;large_interger);</span><br><span class="line">dff = large_interger.QuadPart;</span><br><span class="line"><span class="built_in">QueryPerformanceCounter</span>(&amp;large_interger);</span><br><span class="line">c1 = large_interger.QuadPart;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要计时的部分</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">QueryPerformanceCounter</span>(&amp;large_interger);</span><br><span class="line">c2 = large_interger.QuadPart;</span><br><span class="line">duration = (c2 - c1) * <span class="number">1000</span> / dff;</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;CPU运行时间 = &quot;</span> &lt;&lt; duration &lt;&lt; <span class="string">&quot;ms&quot;</span> &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>

<h4 id="GPU计时"><a href="#GPU计时" class="headerlink" title="GPU计时"></a>GPU计时</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, stop;</span><br><span class="line"><span class="type">float</span> duration_gpu = <span class="number">0.0f</span>;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"><span class="comment">// 要测试的部分</span></span><br><span class="line"><span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(start);</span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;duration_gpu, start, stop);</span><br><span class="line">std::cout &lt;&lt; <span class="string">&quot;持续时间 = &quot;</span> &lt;&lt; duration_gpu &lt;&lt; <span class="string">&quot;ms&quot;</span> &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>

<p> 正常情况下，第一次执行核函数的时间会比第二次慢一些。这是因为GPU在第一次计算时需要warmup。所以想要第一次核函数的执行时间是不精确的。解决方法有以下两种：</p>
<ul>
<li>在计时之前先执行一个warmup函数，warmup函数随便写。这种方法的优点是程序执行时间缩短；缺点是需要在程序中添加一个函数，而且因为GPU乱序并行的执行方式，核函数的两次执行时间并不能完全保持一样。</li>
<li><strong>先执行warmup函数，在循环10遍计时部分。</strong></li>
</ul>
<h3 id="组织并行线程"><a href="#组织并行线程" class="headerlink" title="组织并行线程"></a>组织并行线程</h3><p>介绍每一个线程是怎么确定唯一的索引，然后建立并行计算，并且不同的线程组织形式是怎样影响性能的：</p>
<p>假如计算8<em>8的矩阵加法，使用二维网格（2，4），对应<code>blockDim.x</code>和<code>blockDim.y</code>和二维块（4，2），对应<code>gridDim.x</code>和<code>gridDim.y</code>一维网格（8）和一维块（8），二维网格（2，4）和一维块（8），为什么不指定一个核函数有几个块呢？因为*<em>一个核函数只会对应一个块。</em></em></p>
<p>这里的块指块中线程维度，网格指网格中块的维度。</p>
<h4 id="使用块和线程建立矩阵索引"><a href="#使用块和线程建立矩阵索引" class="headerlink" title="使用块和线程建立矩阵索引"></a>使用块和线程建立矩阵索引</h4><ul>
<li><p>首先区分局部地址（<code>threadIdx.x,threadIDx.y</code>）和全局地址（<code>ix,iy</code>），其中<br>$$<br>ix &#x3D; threadIdx.x + block.x \times blockDim.x \<br>iy &#x3D; threadIdx.x + block.x \times blockDim.x<br>$$</p>
</li>
<li><p>cuda的多线程中的多线程单指令是让每个不同的线程执行相同的代码但是处理的数据是不同的。CUDA常用的做法是让不同的线程对应不同的数据，也就是<strong>用线程的全局标号对应不同组的数据。</strong>线程索引如图所示。</p>
<p><img src="/./../../../../images/image-20240808155953825.png" alt="image-20240808155953825"></p>
</li>
</ul>
<h4 id="使用cuda实现二维矩阵加法"><a href="#使用cuda实现二维矩阵加法" class="headerlink" title="使用cuda实现二维矩阵加法"></a>使用cuda实现二维矩阵加法</h4><p>示例代码分析：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumMatrix</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">float</span> * MatC,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx=ix+iy*ny;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatC[idx]=MatA[idx]+MatB[idx]; <span class="comment">//此处转成一维数组的原因是这里是读取内存中的数据，数组在内存中的存储是线性排列的</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="CUDA执行模型"><a href="#CUDA执行模型" class="headerlink" title="CUDA执行模型"></a>CUDA执行模型</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><h4 id="SM"><a href="#SM" class="headerlink" title="SM"></a>SM</h4><p>GPU中每个SM都能支持数百个线程并发执行，每个GPU通常有多个SM，当一个核函数的网格被启动的时候，多个block会被同时分配给可用的SM上执行。</p>
<p> 当<strong>一个blcok被分配给一个SM</strong>后，<strong>他就只能在这个SM上执行了，不可能重新分配到其他SM上了</strong>，多个线程块可以被分配到同一个SM上。</p>
<h4 id="线程束"><a href="#线程束" class="headerlink" title="线程束"></a>线程束</h4><p>到目前为止基本所有设备都是维持在一个线程束有32个线程，每个SM上有多个block，一个block有多个线程（可以是几百个，但不会超过某个最大值），机器的角度，<strong>在某时刻T，SM上只执行一个线程束</strong>，也就是32个线程在同时同步执行，线程束中的每个线程执行同一条指令，包括有分支的部分。</p>
<p>线程块里不同的线程可能进度都不一样，<strong>但是同一个线程束内的线程拥有相同的进度。</strong></p>
<p>同一个SM上可以有不止一个常驻的线程束，有些在执行，有些在等待，<strong>他们之间状态的转换是不需要开销的。</strong></p>
<h4 id="SIMD-vs-SIMT"><a href="#SIMD-vs-SIMT" class="headerlink" title="SIMD vs SIMT"></a>SIMD vs SIMT</h4><p>SIMD，单指令多数据，不允许每个分支有不同的操作，所有分支必须同时执行相同的指令，必须执行没有例外。</p>
<p>SIMT，单指令多线程，但是SIMT的某些线程可以选择不执行。一个SM在某一个时刻，有32个线程在执行同一条指令，这32个线程可以<strong>选择性执行</strong>，虽然有些可以不执行，<strong>但是他也不能执行别的指令</strong>，需要另外需要执行这条指令的线程执行完，然后再继续下一条。</p>
<ol>
<li>每个线程都有自己的指令地址计数器</li>
<li>每个线程都有自己的寄存器状态</li>
<li>每个线程可以有一个独立的执行路径</li>
</ol>
<h4 id="线程束调度器"><a href="#线程束调度器" class="headerlink" title="线程束调度器"></a>线程束调度器</h4><p>每个SM有n个线程束调度器，和两个指令调度单元，当一个线程块被指定给一个SM时，线程块内的所有线程被分成线程束，线程束选择其中n个线程束，在用指令调度器存储两个线程束要执行的指令，下图以每个SM中有两个线程束调度器为例，线程束调度器和指令调度单元的控制流程如下。</p>
<p><img src="/../images/image-20240809152101683.png" alt="image-20240809152101683"></p>
<h4 id="Hyper-Q技术"><a href="#Hyper-Q技术" class="headerlink" title="Hyper-Q技术"></a>Hyper-Q技术</h4><p>由于 GPU 核数较多, 抢占 GPU 需要保存大量的上下文信息, 开销较大, 所以目前市场上 GPU 都不支持抢占特性. 只用当前任务完成之后, GPU 才能被下个应用程序使用。 在 GPU 虚拟化的环境中, 多用户使用的场景会导致 GPU 进行频繁的任务切换, 可抢占的 GPU 能够防止恶意用户长期占用, 并且 能够实现用户优先级权限管理。</p>
<p>Hyper-Q：允许多个CPU 线程或进程同时加载任务到一个GPU上， 实现CUDA kernels的并发执行 –- 硬件特性</p>
<p>参考文章：<a target="_blank" rel="noopener" href="https://damonyi.cc/2020/12/10/GPU%E4%B8%AD%E7%9A%84Hyper-Q%E6%8A%80%E6%9C%AF/">GPU中的Hyper-Q技术 | 云里雾里 (damonyi.cc)</a></p>
<h4 id="使用Profile进行优化"><a href="#使用Profile进行优化" class="headerlink" title="使用Profile进行优化"></a>使用Profile进行优化</h4><p>使用性能分析工具。</p>
<ul>
<li>nvvp</li>
<li>nvprof</li>
</ul>
<p>限制内核性能的主要包括但不限于以下因素</p>
<ul>
<li>存储带宽</li>
<li>计算资源</li>
<li>指令和内存延迟</li>
</ul>
<h3 id="线程束执行的本质"><a href="#线程束执行的本质" class="headerlink" title="线程束执行的本质"></a>线程束执行的本质</h3><h4 id="线程束和线程块"><a href="#线程束和线程块" class="headerlink" title="线程束和线程块"></a>线程束和线程块</h4><p>当一个核函数执行时，可分为以下几个步骤：、</p>
<ul>
<li><p>一个网格被启动（每个核函数对应一个网格），一个网格包含<code>gridDim.x * gridDim.y * gridDim.z</code>个线程块。</p>
</li>
<li><p>线程块被分配到SM中（一个block只能被分配到一个SM中，一个SM可以执行好几个block）。</p>
</li>
<li><p>分配到SM中后，线程块将被分为n个线程束，一个线程束包括32个线程（目前硬件规定的值），在一个线程束中，所有线程按照单指令多线程SIMT的方式执行，每一步执行相同的指令，但是处理的数据为私有的数据，也就是数据不同。当线程块中的线程数不能被32整除时，n向上取整。</p>
</li>
<li><p>当一个线程块中有128个线程的时候，其分配到SM上执行时，会分成4个块，按照<strong>线程编号将线程分配到线程束</strong>中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">warp0: thread  0,........thread31</span><br><span class="line">warp1: thread 32,........thread63</span><br><span class="line">warp2: thread 64,........thread95</span><br><span class="line">warp3: thread 96,........thread127</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="/../images/image-20240810175913252.png" alt="image-20240810175913252"></p>
<p>线程束和线程块，一个是硬件层面的线程集合，一个是逻辑层面的线程集合，我们编程时为了程序正确，必须从逻辑层面计算清楚，但是为了得到更快的程序，硬件层面是我们应该注意的。</p>
<p><strong>补充：一个线程束中的threadIdx.x 是连续变化的</strong></p>
<h4 id="线程束分化"><a href="#线程束分化" class="headerlink" title="线程束分化"></a>线程束分化</h4><p>假设这段代码是核函数的一部分，那么当一个线程束的32个线程执行这段代码的时候，如果其中16个执行if中的代码段，而另外16个执行else中的代码块，同一个线程束中的线程，执行不同的指令，这叫做线程束的分化。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (con)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//do something</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//do something</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>线程束的分化会带来性能的削弱，因为分配命令的调度器就一个，两个分支就需要两个指令周期才能执行完。</p>
<p>优化方法：这就使得我们<strong>根据线程编号来设计分支</strong>是可以的，补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。线程束内的线程是可以被我们控制的，那么我们就把都执行if的线程塞到一个线程束中，或者让一个线程束中的线程都执行if，另外线程都执行else的这种方式可以将效率提高很多。示例如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">mathKernel2</span><span class="params">(<span class="type">float</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="type">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="type">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> ((tid/warpSize) % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意<code>warpSize</code>这个常量值为32，可以使用这个优化程序。</p>
<h4 id="资源分配"><a href="#资源分配" class="headerlink" title="资源分配"></a>资源分配</h4><p>一个SM上被分配多少个线程块和线程束取决于SM中可用的寄存器和共享内存，以及内核需要的寄存器和共享内存大小。当kernel占用的资源<strong>较少</strong>，那么更多的线程（这是线程越多线程束也就越多）处于<strong>活跃状态</strong>，相反则线程越少。</p>
<p>特别是当SM内的资源没办法处理一个完整块，那么程序将无法启动。</p>
<p>当寄存器和共享内存分配给了线程块，这个线程块处于活跃状态，所包含的线程束称为活跃线程束。<br>活跃的线程束又分为三类：</p>
<ul>
<li>选定的线程束</li>
<li>阻塞的线程束</li>
<li>符合条件的线程束</li>
</ul>
<p>当SM要执行某个线程束的时候，执行的这个线程束叫做选定的线程束，准备要执行的叫符合条件的线程束，如果线程束不符合条件还没准备好就是阻塞的线程束。<br>满足下面的要求，线程束才算是符合条件的：</p>
<ul>
<li>32个CUDA核心可以用于执行</li>
<li>执行所需要的资源全部就位</li>
</ul>
<h4 id="延迟隐藏"><a href="#延迟隐藏" class="headerlink" title="延迟隐藏"></a>延迟隐藏</h4><p>延迟隐藏，延迟是什么，就是当你让计算机帮你算一个东西的时候计算需要用的时间。延迟影藏就是通过添加任务优化让计算机的延迟缩小，原来的延迟和缩小后的延迟之差就是延迟隐藏。</p>
<p>所以最大化是要最大化硬件，尤其是计算部分的硬件满跑，都不闲着的情况下利用率是最高的，总有人闲着，利用率就会低很多，即最大化功能单元的利用率。利用率与常驻线程束直接相关。<br>硬件中线程调度器负责调度线程束调度，当每时每刻都有可用的线程束供其调度，这时候可以达到计算资源的完全利用，以此来保证通过其他常驻线程束中发布其他指令的，可以隐藏每个指令的延迟。</p>
<p>对于指令的延迟，通常分为两种：</p>
<ul>
<li>算术指令</li>
<li>内存指令</li>
</ul>
<p>算数指令延迟是一个算术操作从开始，到产生结果之间的时间，这个时间段内只有某些计算单元处于工作状态，而其他逻辑计算单元处于空闲。<br>内存指令延迟很好理解，当产生内存访问的时候，计算单元要等数据从内存拿到寄存器，这个周期是非常长的。<br>延迟：</p>
<ul>
<li>算术延迟 10~20 个时钟周期</li>
<li>内存延迟 400~800 个时钟周期</li>
</ul>
<p>那么至少需要多少线程，线程束来保证最小化延迟呢？<br>$$<br>所需线程束 &#x3D; 延迟 \times 吞吐量<br>$$<br>吞吐量是指实际操作过程中每分钟处理多少个指令。</p>
<p>另外有两种方法可以提高并行：</p>
<ul>
<li>指令级并行(ILP): 一个线程中有很多独立的指令</li>
<li>线程级并行(TLP): 很多并发地符合条件的线程</li>
</ul>
<p>我们的根本目的是把计算资源，内存读取的带宽资源全部使用满，这样就能达到理论的最大效率。</p>
<p>那么我们怎么样确定一个线程束的下界呢，使得当高于这个数字时SM的延迟能充分的隐藏，其实这个公式很简单，也很好理解，就是SM的计算核心数乘以单条指令的延迟，<br><strong>比如32个单精度浮点计算器，每次计算延迟20个时钟周期，那么我需要最少 32x20 &#x3D;640 个线程使设备处于忙碌状态。</strong></p>
<h4 id="占用率"><a href="#占用率" class="headerlink" title="占用率"></a>占用率</h4><p>占用率是一个SM种活跃的线程束的数量，占SM最大支持线程束数量的比。</p>
<h4 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h4><p>同步的目的是为了避免内存竞争。</p>
<p>CUDA同步这里只讲两种：</p>
<ul>
<li>线程块内同步</li>
<li>系统级别(<code>cudaDeviceSynchronize()</code>)</li>
</ul>
<p>块级别的就是同一个块内的线程会同时停止在某个设定的位置，使用</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncthread();</span><br></pre></td></tr></table></figure>

<p>这个函数<strong>只能同步同一个块内的线程</strong>，不能同步不同块内的线程，想要同步不同块内的线程，就只能让核函数执行完成，控制程序交换主机，这种方式来同步所有线程。</p>
<h3 id="使用工具分析核函数执行效率"><a href="#使用工具分析核函数执行效率" class="headerlink" title="使用工具分析核函数执行效率"></a>使用工具分析核函数执行效率</h3><p><strong>Visual Profiler 和 nvprof 将在未来的 CUDA 版本中被弃用。</strong>NVIDIA Volta 平台是完全支持这些工具的最后一个架构。建议使用下一代工具， <a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-systems">NVIDIA Nsight Systems</a> 用于 GPU 和 CPU 采样和跟踪，以及 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-compute">NVIDIA Nsight Compute</a> 用于 GPU 内核分析 </p>
<h4 id="Nsight-Systems-vs-Nsight-Compute"><a href="#Nsight-Systems-vs-Nsight-Compute" class="headerlink" title="Nsight Systems vs Nsight Compute"></a>Nsight Systems vs Nsight Compute</h4><p>Nsight Systems：是用于系统级别性能分析和优化的工具，可以用于分析整个系统中的CPU、GPU和内存等资源的使用情况。用于调试和优化在复杂系统环境中运行的大型应用程序，尤其是需要同时关注多个硬件资源的情况。</p>
<p>Nsight Compute：Nsight Compute是一款专门针对GPU的内核级（kernel-level）分析工具。它用于深入分析和优化CUDA内核的性能。提供详细的GPU内核性能指标，包括内存带宽、指令吞吐量、线程效率等。可以查看和分析每个CUDA内核在不同硬件单元上的性能数据，如寄存器使用、缓存命中率等。自动识别CUDA内核中的性能瓶颈，并提供优化建议。用于CUDA开发人员对GPU内核进行精细调优，找到和解决特定的内核性能瓶颈。</p>
<h4 id="Nsight-Compute"><a href="#Nsight-Compute" class="headerlink" title="Nsight Compute"></a>Nsight Compute</h4><p>原理： Nsight Compute将其测量库插入到应用程序进程中，从而允许分析器拦截与 CUDA 用户模式驱动程序的通信。此外，当检测到内核启动时，库可以从 GPU 收集请求的性能指标。然后将结果传输回前端。</p>
<h3 id="避免分支化"><a href="#避免分支化" class="headerlink" title="避免分支化"></a>避免分支化</h3><h4 id="普通优化"><a href="#普通优化" class="headerlink" title="普通优化"></a>普通优化</h4><p>基本思想是利用线程编号，避免编号相近的线程被执行不同的指令，这样会造成线程束分化。</p>
<h4 id="for循环优化"><a href="#for循环优化" class="headerlink" title="for循环优化"></a>for循环优化</h4><p>对于<code>for</code>循环，<code>for</code>循环也会造成线程束分化，因为下一次循环需要上一次计算结果的话，那么就会造成分化</p>
<p>改善<code>for</code>循环的分化方法是修改循环体的内容，让<code>for</code>循环的步长增加。</p>
<h4 id="线程束最后32个线程优化"><a href="#线程束最后32个线程优化" class="headerlink" title="线程束最后32个线程优化"></a>线程束最后32个线程优化</h4><p>对于循环中线程束的最后32个线程，可以让不足32个线程的步骤，每一步跑满32个线程。</p>
<p>例如：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">volatile</span> <span class="type">int</span> *vmem = idata;</span><br><span class="line">vmem[tid]+=vmem[tid+<span class="number">32</span>];</span><br><span class="line">vmem[tid]+=vmem[tid+<span class="number">16</span>];</span><br><span class="line">vmem[tid]+=vmem[tid+<span class="number">8</span>];</span><br><span class="line">vmem[tid]+=vmem[tid+<span class="number">4</span>];</span><br><span class="line">vmem[tid]+=vmem[tid+<span class="number">2</span>];</span><br><span class="line">vmem[tid]+=vmem[tid+<span class="number">1</span>];</span><br></pre></td></tr></table></figure>

<p>注：因为我们的CUDA内核从内存中读数据到寄存器，然后进行加法都是同步进行的，也就是17号线程和1号线程同时读33号和17号的内存，这样17号即便在下一步修改，也不影响1号线程寄存器里面的值了。</p>
<p><code>volatile int</code>类型变量是控制变量结果写回到内存，而不是存在共享内存，或者缓存中，因为下一步的计算马上要用到它，如果写入缓存，可能造成下一步的读取会读到错误的数据</p>
<p><code>id+16</code>要用到<code>tid+32</code>的结果，会不会有其他的线程造成内存竞争，答案是不会的，因为一个线程束，执行的进度是完全相同的，当执行 <code>tid+32</code>的时候，这32个线程都在执行这步，而不会有任何本线程束内的线程会进行到下一句。</p>
<h2 id="CUDA内存模型"><a href="#CUDA内存模型" class="headerlink" title="CUDA内存模型"></a>CUDA内存模型</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><h4 id="内存层次结构特点"><a href="#内存层次结构特点" class="headerlink" title="内存层次结构特点"></a>内存层次结构特点</h4><p>内存速度从快到慢，内存大小从小到大：寄存器（Registers）、缓存（Caches）、内存（Main Memory）、硬盘（Disk Memory）</p>
<p>可编程内存VS不可编程内存：CPU内存中，一级二级缓存都是不可编程的存储设备。</p>
<p>CUDA的存储器可以大致分为两类：</p>
<ul>
<li><strong>板载显存（On-board memory）(DRAM)</strong></li>
<li><strong>片上内存（On-chip memory）</strong></li>
</ul>
<p>其中板载显存主要包括全局内存（global memory）、本地内存（local memory）、常量内存（constant memory）、纹理内存（texture memory）等，片上内存主要包括寄存器（register）和共享内存（shared memory）。</p>
<h4 id="CUDA内存模型-1"><a href="#CUDA内存模型-1" class="headerlink" title="CUDA内存模型"></a>CUDA内存模型</h4><p>GPU内存设备：寄存器、共享内存、本地内存、常量内存、纹理内存、全局内存。</p>
<p>区别：CUDA中<strong>每个线程</strong>都有自己的私有的<strong>本地内存</strong>；<strong>线程块</strong>有自己的<strong>共享内存</strong>，对线程块内所有线程可见；所有线程都能访问读取<strong>常量内存和纹理内存，但是不能写</strong>，因为他们是只读的；全局内存，常量内存和纹理内存空间有不同的用途。对于一个应用来说，全局内存，常量内存和纹理内存有相同的生命周期。</p>
<h4 id="寄存器-1"><a href="#寄存器-1" class="headerlink" title="寄存器"></a>寄存器</h4><p>CPU：只有<strong>当前在计算的变量</strong>存储在寄存器中，其余在主存中，使用时传输至寄存器。</p>
<p>GPU：当我们在核函数内不加修饰的<strong>声明一个变量</strong>，此变量就存储在寄存器中，在核函数中定义的有常数长度的数组也是在寄存器中分配地址的。如果一个线程里面的变量太多，以至于寄存器完全不够呢？这时候寄存器发生溢出，本地内存就会过来帮忙存储多出来的变量，这种情况会对效率产生非常负面的影响。</p>
<p>寄存器对于<strong>每个线程是私有的</strong>，寄存器通常保存被频繁使用的私有变量，注意这里的变量也一定不能使共有的，不然的话彼此之间不可见，就会导致大家同时改变一个变量而互相不知道。</p>
<p>为了避免寄存器溢出，可以在核函数的代码中配置额外的信息来辅助编译器优化，比如：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span></span><br><span class="line">__lauch_bounds__(maxThreadaPerBlock,minBlocksPerMultiprocessor) <span class="comment">//这里面在核函数定义前加了一个 关键字 lauch_bounds，然后他后面对应了两个变量：maxThreadaPerBlock：线程块内包含的最大线程数，线程块由核函数来启动minBlocksPerMultiprocessor：可选参数，每个SM中预期的最小的常驻内存块参数。</span></span><br><span class="line"><span class="built_in">kernel</span>(...) &#123;</span><br><span class="line">    <span class="comment">/* kernel code */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="本地内存"><a href="#本地内存" class="headerlink" title="本地内存"></a>本地内存</h4><p>函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量将存储在本地内存中，编译器可能存放在本地内存中的变量有以下几种：</p>
<ul>
<li>使用未知索引引用的本地数组</li>
<li>可能会占用大量寄存器空间的较大本地数组或者结构体</li>
<li>任何不满足核函数寄存器限定条件的变量</li>
</ul>
<p>本地内存实质上是和全局内存一样在同一块存储区域当中的，其访问特点——高延迟，低带宽。<br>对于2.0以上的设备，本地内存存储在<strong>每个SM的一级缓存，或者设备的二级缓存上</strong>。</p>
<h4 id="共享内存-1"><a href="#共享内存-1" class="headerlink" title="共享内存"></a>共享内存</h4><p>修饰符：<code>__share__</code>。</p>
<p>特点：每个SM都有一定数量的由线程块分配的共享内存，共享内存是片上内存，跟主存相比，速度要快很多。</p>
<p>注意：不要因为过度使用共享内存，而导致SM上活跃的线程束减少，也就是说，一个线程块使用的共享内存过多，导致更过的线程块没办法被SM启动，<strong>这样影响活跃的线程束数量</strong>。</p>
<p>生命周期：线程块运行开始，此块的共享内存被分配，当此块结束，则共享内存被释放。<strong>取决于线程块。</strong></p>
<p>如何避免内存竞争：使用同步语句：<code>void __syncthreads();</code>，但不要频繁使用，会影响内核执行效率。此语句相当于在线程块执行时各个线程的一个障碍点，<strong>当块内所有线程都执行到本障碍点的时候才能进行下一步的计算</strong>，这样可以设计出避免内存竞争的共享内存使用程序。</p>
<p>注：在硬件结构中，SM中的一级缓存，和共享内存共享一个片上内存，他们通过静态划分，划分彼此的容量，运行时可以通过下面语句进行设置：<a href="#SetCacheConfig">使用</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFuncSetCacheConfig</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * func,<span class="keyword">enum</span> cudaFuncCache)</span></span>;</span><br></pre></td></tr></table></figure>

<h4 id="常量内存-1"><a href="#常量内存-1" class="headerlink" title="常量内存"></a>常量内存</h4><p>修饰符：<code>__constant__</code></p>
<p>特点：量内存在核函数外，全局范围内声明，对于所有设备，只可以声明64k的常量内存，常量内存静态声明，并对同一编译单元中的所有核函数可见。<strong>被主机端初始化后不能被核函数修改，可以被主机端代码修改。</strong></p>
<p>初始化：<a href="#cudaMemcpyToSymbol">使用</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpyToSymbol</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* symbol,<span class="type">const</span> <span class="type">void</span> *src,<span class="type">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure>

<p>当线程束中所有线程<strong>都从相同的地址取数据时</strong>，常量内存表现较好，比如执行某一个多项式计算，系数都存在常量内存里效率会非常高，但是如果不同的线程取不同地址的数据，常量内存就不那么好了，因为常量内存的读取机制是：<strong>一次读取会广播给所有线程束内的线程。</strong></p>
<h4 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h4><p>纹理内存在每个SM的只读缓存中缓存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化。 </p>
<h4 id="全局内存-1"><a href="#全局内存-1" class="headerlink" title="全局内存"></a>全局内存</h4><p>一般在主机端代码里定义，也可以在设备端定义，不过需要加修饰符，只要不销毁，是和应用程序同生命周期的。</p>
<p>静态声明：<code>__device__</code></p>
<p>动态声明：<code>cudaMalloc</code></p>
<p>注1：当有多个核函数同时执行的时候，如果使用到了同一全局变量，应注意内存竞争。</p>
<p>注2：全局内存访问是对齐，也就是一次要读取指定大小（32，64，128）整数倍字节的内存。</p>
<h4 id="GPU缓存"><a href="#GPU缓存" class="headerlink" title="GPU缓存"></a>GPU缓存</h4><p>与CPU缓存类似，GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存：</p>
<ul>
<li>一级缓存：每个SM都有一个一级缓存</li>
<li>二级缓存：所有SM公用一个二级缓存</li>
<li>只读常量缓存</li>
<li>只读纹理缓存</li>
</ul>
<p>一级二级缓存的作用都是被用来存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。 </p>
<p>每个SM有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能。</p>
<h4 id="静态全局内存"><a href="#静态全局内存" class="headerlink" title="静态全局内存"></a>静态全局内存</h4><p>使用示例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__device <span class="type">float</span> devData </span><br><span class="line"><span class="type">float</span> value=<span class="number">3.14f</span>;</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(devData,&amp;value,<span class="built_in">sizeof</span>(<span class="type">float</span>)); <span class="comment">// 从主机端复制静态变量到设备端</span></span><br><span class="line">....</span><br><span class="line"><span class="built_in">cudaMemcpyFromSymbol</span>(&amp;value,devData,<span class="built_in">sizeof</span>(<span class="type">float</span>)); <span class="comment">// 从设备端复制静态变量到主机端</span></span><br></pre></td></tr></table></figure>

<p>注1：<code>cudaMemcpyToSymbol(devData,&amp;value,sizeof(float));</code>函数原型中使用<code>void* symbol</code>，这里为什么可以使用<code>__device__ float devData</code>。这是因为<strong>设备变量在代码中定义的时候其实就是一个指针</strong>，这个指针指向何处，<strong>主机端是不知道的，指向的内容也不知道</strong>，想知道指向的内容，唯一的办法还是通过显式的办法传输过来。</p>
<p>注2：不可以直接使用<code>cudaMemcpy</code>，这是动态复制方法，若要使用。需要首先使用<code>cudaGetSymbolAddress((void**)&amp;dptr,devData)</code>获得设备变量地址。主机端是不可以直接对设备端的变量取地址的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *dptr=<span class="literal">NULL</span>;</span><br><span class="line"><span class="built_in">cudaGetSymbolAddress</span>((<span class="type">void</span>**)&amp;dptr,devData);</span><br><span class="line"><span class="built_in">cudaMemcpy</span>(dptr,&amp;value,<span class="built_in">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure>

<h3 id="内存管理-1"><a href="#内存管理-1" class="headerlink" title="内存管理"></a>内存管理</h3><h4 id="内存分配和释放"><a href="#内存分配和释放" class="headerlink" title="内存分配和释放"></a>内存分配和释放</h4><p>分配</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="type">void</span> **devPtr, <span class="type">size_t</span> nByte)</span></span>; <span class="comment">//第一个参数是指针的指针</span></span><br></pre></td></tr></table></figure>

<p>初始化内存</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="type">void</span> *devPtr,<span class="type">int</span> value, <span class="type">size_t</span> count)</span></span>; <span class="comment">// 这一段内存的值都分配为value</span></span><br></pre></td></tr></table></figure>

<p>释放</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="type">void</span> *devPtr)</span></span></span><br></pre></td></tr></table></figure>

<h4 id="内存传输"><a href="#内存传输" class="headerlink" title="内存传输"></a>内存传输</h4><p>传输</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="type">void</span> *dst,<span class="type">const</span> <span class="type">void</span> * src,<span class="type">size_t</span> count,<span class="keyword">enum</span> cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure>

<p>传输类型</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>CPU和GPU之间通信要经过PCIe总线，总线的理论峰值要低很多——8GB&#x2F;s左右，也就是说所，管理不当，算到半路需要从主机读数据，那效率瞬间全挂在PCIe上了。<strong>CUDA编程需要大家减少主机和设备之间的内存传输。</strong></p>
<h4 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h4><p>主机内存采用分页式管理，通俗的说法就是操作系统把物理内存分成一些“页”，然后给一个应用程序一大块内存，但是这一大块内存可能在一些不连续的页上，应用只能看到虚拟的内存地址，而操作系统可能随时更换物理地址的页（从原始地址复制到另一个地址）但是应用是不会觉得，但是从主机传输到设备上的时候，如果此时发生了页面移动，对于传输操作来说是致命的。</p>
<p>因此CUDA传输内存时通过两种方式解决：</p>
<ul>
<li>正常分配内存：锁页-复制到固定内存-复制到设备</li>
<li>固定内存：直接分配固定的主机内存，将主机源数据复制到固定内存上，然后从固定内存传输数据到设备上</li>
</ul>
<p>分配固定内存，这样就是的传输带宽变得高很多</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="type">void</span> ** devPtr,<span class="type">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>

<p>固定的主机内存释放</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFreeHost</span><span class="params">(<span class="type">void</span> *ptr)</span></span></span><br></pre></td></tr></table></figure>

<p><strong>固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。尽量使用流来使内存传输和计算之间同时进行</strong>。</p>
<h4 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h4><p>GPU线程可以直接访问零拷贝内存，<strong>这部分内存在主机内存里面，因此零拷贝内存是实现了设备访问主机内存</strong>。</p>
<p>CUDA核函数使用零拷贝内存有以下几种情况：</p>
<ul>
<li>当设备内存不足的时候可以利用主机内存</li>
<li>避免主机和设备之间的显式内存传输</li>
<li>提高PCIe传输率</li>
</ul>
<p><strong>零拷贝内存是固定内存，不可分页。</strong></p>
<p>创建零拷贝内存</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="type">void</span> ** pHost,<span class="type">size_t</span> count,<span class="type">unsigned</span> <span class="type">int</span> flags)</span></span></span><br></pre></td></tr></table></figure>

<p>标志参数可选值</p>
<ul>
<li>cudaHostAllocDefalt：等同于<code>cudaMallocHost</code>，分配主机内存</li>
<li>cudaHostAllocPortable：返回能被所有CUDA上下文使用的<strong>固定内存</strong></li>
<li>cudaHostAllocWriteCombined：返回<strong>写结合内存</strong>，在某些设备上这种内存传输效率更高</li>
<li>cudaHostAllocMapped：<strong>产生零拷贝内存</strong></li>
</ul>
<p>创建完成之后设备<strong>还是不能通过</strong><code>phost</code>指针来访问对应的主机内存地址，需要先获得另一个地址</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="type">void</span> ** pDevice,<span class="type">void</span> * pHost,<span class="type">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>

<p><code>pDevice</code>就是设备上访问主机零拷贝内存的指针，此处<code>flag</code>必须设置为0。</p>
<p>零拷贝内存可以当做<strong>比设备主存储器更慢的</strong>一个设备。</p>
<h4 id="统一虚拟寻址（UVA）"><a href="#统一虚拟寻址（UVA）" class="headerlink" title="统一虚拟寻址（UVA）"></a>统一虚拟寻址（UVA）</h4><p>设备内存和主机内存被映射到同一虚拟内存地址中。</p>
<p><img src="/../images/image-20240828153147100.png" alt="image-20240828153147100"></p>
<p>通过UVA，cudaHostAlloc函数分配的固定主机内存具有相同的主机和设备地址，可以直接将返回的地址传递给核函数。也就是说<strong>不需要上面的那个获得设备上访问零拷贝内存的函数了</strong>。（<code>cudaError_t cudaHostGetDevicePointer(void ** pDevice,void * pHost,unsigned flags)</code>）</p>
<h4 id="统一内存寻址"><a href="#统一内存寻址" class="headerlink" title="统一内存寻址"></a>统一内存寻址</h4><p>统一内存中创建一个托管内存池（CPU上有，GPU上也有），内存池中已分配的空间<strong>可以通过相同的指针直接被CPU和GPU访问</strong>，底层系统在统一的内存空间中自动的进行设备和主机间的传输。</p>
<p>托管内存是指底层系统自动分配的统一内存，未托管内存就是我们自己分配的内存，这时候对于核函数，可以传递给他两种类型的内存，已托管和未托管内存，可以同时传递。<br>托管内存可以是静态的，也可以是动态的，添加 <code>managed</code>关键字修饰托管内存变量。静态声明的托管内存作用域是文件，这一点可以注意一下。</p>
<p>托管内存分配：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocManaged</span><span class="params">(<span class="type">void</span> ** devPtr,<span class="type">size_t</span> size,<span class="type">unsigned</span> <span class="type">int</span> flags=<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure>

<h3 id="内存访问模式——全局内存"><a href="#内存访问模式——全局内存" class="headerlink" title="内存访问模式——全局内存"></a>内存访问模式——全局内存</h3><p>CUDA内存访问也是以线程束为基本单位发布和执行的，存储也一致。以下从线程束的内存访问进行描述：</p>
<p>核函数运行时需要从全局内存（DRAM）中读取数据，只有两种粒度（最小单位），也被称为<strong>缓存粒度</strong></p>
<ul>
<li>128字节</li>
<li>32字节</li>
</ul>
<p>核函数运行时每次读内存，哪怕是读一个字节的变量，也要读128字节，或者32字节，而具体是到底是32还是128还是要看访问方式。</p>
<p>CUDA是支持通过编译指令停用一级缓存的。<strong>如果启用一级缓存，那么每次从DRAM上加载数据的粒度是128字节，如果不适用一级缓存，只是用二级缓存，那么粒度是32字节。</strong></p>
<p>原因：L1缓存会预取更多的数据，因为相对于L2缓存，它离执行单元更近，缓存命中率更高，能更好地利用较大的加载粒度。此外，由于L1缓存更贴近线程执行，预取较大数据块可以减少未来访问时的延迟。</p>
<p>原因：当一个SM中正在被执行的某个线程需要访问内存，那么，和它同线程束的其他31个线程也要访问内存，这个基础就表示，即使每个线程只访问一个字节，那么在执行的时候，只要有内存请求，至少是32个字节，所以不使用一级缓存的内存加载（每个SM内部的L1缓存），一次粒度是32字节而不是更小。</p>
<h4 id="对齐和合并访问"><a href="#对齐和合并访问" class="headerlink" title="对齐和合并访问"></a>对齐和合并访问</h4><p>内存事务：从内核函数发起请求，到硬件响应返回数据这个过程。</p>
<p>对齐内存访问：当一个内存事务的首个访问地址是缓存粒度（32或128字节）的整数倍的时候。非对齐访问会造成内存浪费。</p>
<p>合并访问：<strong>当一个线程束内的线程访问的内存都在一个内存块里的时候，并且是对齐的，这些访问可以被合并为一个内存事务</strong>。为了提高内存访问效率，GPU尝试将一个Warp中的多个线程的内存访问请求合并为一个或几个内存事务。这意味着，如果一个Warp的32个线程访问的内存地址是连续的且对齐良好，GPU可以通过一次内存事务将所有数据加载到缓存或寄存器中。</p>
<p>对齐合并访问的状态是理想化的，也是最高速的访问方式，为了最大化全局内存访问的理想状态，尽量将线程束访问内存组织成对齐合并的方式，这样的效率是最高的。</p>
<p>优化关键：<strong>用最少的事务次数满足最多的内存请求。事务数量和吞吐量的需求随设备的计算能力变化。</strong></p>
<h4 id="全局内存读取"><a href="#全局内存读取" class="headerlink" title="全局内存读取"></a>全局内存读取</h4><p>SM加载数据，根据不同的设备和类型分为三种路径：</p>
<ul>
<li><p>一级和二级缓存</p>
</li>
<li><p>常量缓存</p>
</li>
<li><p>只读缓存</p>
</li>
</ul>
<p>编译器禁用一级缓存</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xptxas -dlcm=cg</span><br></pre></td></tr></table></figure>

<p>编译器启用一级缓存</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xptxas -dlcm=ca</span><br></pre></td></tr></table></figure>

<p>当一级缓存被禁用的时候，对全局内存的加载请求直接进入二级缓存，如果二级缓存缺失，则由DRAM完成请求。</p>
<p>只读缓存：只读缓存最初是留给纹理内存加载用的，在3.5以上的设备，只读缓存也支持使用全局内存加载代替一级缓存。也<strong>就是说3.5以后的设备，可以通过只读缓存从全局内存中读数据了</strong>。<strong>只读缓存粒度32字节</strong>，对于分散读取，细粒度优于一级缓存。</p>
<p>从只读缓存中读取：</p>
<ul>
<li>使用函数_ldg</li>
<li>在在间接引用的指针上使用修饰符</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out[idx] = _ldg(&amp;in[idx]);</span><br></pre></td></tr></table></figure>

<h4 id="全局内存写入"><a href="#全局内存写入" class="headerlink" title="全局内存写入"></a>全局内存写入</h4><p>一级缓存不能用在 Fermi 和 Kepler GPU上进行存储操作，发送到设备前，只经过二级缓存，存储操作在32个字节的粒度上执行。</p>
<h4 id="结构体数组与数组结构体"><a href="#结构体数组与数组结构体" class="headerlink" title="结构体数组与数组结构体"></a>结构体数组与数组结构体</h4><ol>
<li><p>结构体在内存中的表现</p>
<p>结构中的成员在内存里对齐的依次排开</p>
</li>
<li><p>数组结构体（AOS）</p>
<p>一个数组，每个元素都是结构体</p>
</li>
<li><p>结构体数组（SOA）</p>
<p>结构体的成员都是数组</p>
</li>
</ol>
<p>注1：CUDA对细粒度数组是非常友好的，但是对粗粒度如<strong>结构体组成的数组</strong>就不太友好，会导致内存访问利用率低。因为在访问某个结构体成员时，当32个线程同时访问时，AOS是不连续的，SOA是连续的，因此CUDA对SOA更友好。</p>
<h3 id="共享内存-2"><a href="#共享内存-2" class="headerlink" title="共享内存"></a>共享内存</h3><p>几个内存之间的关系</p>
<p><img src="/../images/image-20240906151243038.png" alt="image-20240906151243038"></p>
<p>生存周期：共享内存是在他所属的线程块被执行时建立，线程块执行完毕后共享内存释放，线程块和他的共享内存有相同的生命周期。</p>
<p>共享内存越大，或者块使用的共享内存越小，那么线程块级别的并行度就越高。</p>
<h4 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h4><p>关键字</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__share__ <span class="type">float</span> value;  <span class="comment">//声明共享内存变量</span></span><br></pre></td></tr></table></figure>

<p>声明共享内存数组</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__share__ <span class="type">float</span> MyArrary[x][y]; <span class="comment">//声明二维数组</span></span><br></pre></td></tr></table></figure>

<p>这里的<code>x</code>和<code>y</code>和C++声明数组一样，不能是变量，在编译的时候要是一个确定值。</p>
<p>共享内存的声明可以在核函数内，也可以在核函数外。</p>
<p>共享内存动态数组声明，使用<code>extern</code>关键字</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __share__ <span class="type">float</span> value[]; <span class="comment">//动态数组声明</span></span><br><span class="line">kernel&lt;&lt;&lt;grid,block,<span class="function">isize*<span class="title">sizeof</span><span class="params">(<span class="type">float</span>)</span>&gt;&gt;&gt;<span class="params">(...)</span></span>; <span class="comment">//核函数申请时需要更改，isize是动态数组的长度</span></span><br></pre></td></tr></table></figure>

<h4 id="访问模式"><a href="#访问模式" class="headerlink" title="访问模式"></a>访问模式</h4><p>内存存储体</p>
<p>共享内存有个特殊的形式是，分为32个同样大小的内存模型（对应32个线程），称为存储体，可以同时访问。如果32个线程同时访问32个不同的存储体，则不会产生冲突。</p>
<p>存储体冲突</p>
<p>当多个线程要访问一个存储体的时候，冲突就发生了，注意这里是说访问同一个存储体，而不是同一个地址，访问同一个地址不存在冲突（广播形式）。</p>
<p>广播访问是所有线程访问一个地址，这时候，一个内存事务执行完毕后，一个线程得到了这个地址的数据<strong>，他会通过广播的形式告诉其他所有线程，</strong>虽然这个延迟相比于完全的并行访问并不慢，但是他只读取了一个数据，带宽利用率很差。</p>
<p>共享内存的存储体的访问模式</p>
<p>根据存储体的宽度（计算能力2.x的宽度为4字节，计算能力3.x的宽度为8字节），假如有1024字节的数据，存储体宽度为8字节，存储的时候，第一个8字节数据被放在第一个存储体，第二个8字节数据被放在第二个存储体，以此类推。等到32个存储体存完一遍后，再从第一个开始存储。</p>
<p>存储体索引<br>$$<br>存储体索引 &#x3D; \frac{字节地址 ÷ 存储体宽度}{存储体数} \quad \bmod 存储体数<br>$$</p>
<p>如何解决存储体冲突——内存填充</p>
<p>假设存储体大小是5个（实际上是32个），当分配内存时声明，此时刚好填满</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__share__ <span class="type">int</span> a[<span class="number">5</span>][<span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<p><img src="/../images/image-20240910091357014.png" alt="image-20240910091357014"></p>
<p>假如这时候访问<code>bank 0</code>的时候会有5线程的冲突，为了解决这个冲突，可以声明为</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__share__ <span class="type">int</span> a[<span class="number">5</span>][<span class="number">6</span>]</span><br></pre></td></tr></table></figure>

<p>这样会在编程的时候加入一行填充物，然后编译器就会将二维数组重新分配内存，可以让所有元素都错开，如图所示。</p>
<p><img src="/../images/image-20240910091857453.png" alt="image-20240910091857453"></p>
<p>访问模式查询：4字节 or 8字节</p>
 <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceGetSharedMemConfig</span><span class="params">(cudaSharedMemConfig * pConfig)</span>;</span><br></pre></td></tr></table></figure>

<p>返回<code>pConfig</code>的值</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeFourByte</span><br><span class="line">cudaSharedMemBankSizeEightByte</span><br></pre></td></tr></table></figure>

<p>配置存储体大小</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceSetShareMemConfig</span><span class="params">(cudaSharedMemConfig config)</span>;</span><br></pre></td></tr></table></figure>

<p><code>config</code>的值</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeDefault</span><br><span class="line">cudaSharedMemBankSizeFourByte</span><br><span class="line">cudaSharedMemBankSizeEightByte</span><br></pre></td></tr></table></figure>

<h4 id="配置共享内存"><a href="#配置共享内存" class="headerlink" title="配置共享内存"></a>配置共享内存</h4><p>每个SM上有64KB的片上内存，共享内存和L1共享这64KB，并且可以配置。此内存更多给核函数使用，多配置共享内存，给更多寄存器使用，多配置L1缓存。一级缓存和共享内存都在同一个片上，但是行为大不相同，共享内存靠的的是存储体来管理数据，而L1则是通过缓存行进行访问。我们对共享内存有绝对的控制权，但是L1的删除工作是硬件完成的。</p>
<p>配置函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceSetCacheConfig</span><span class="params">(cudaFuncCache cacheConfig)</span>;</span><br></pre></td></tr></table></figure>

<p>配置参数<code>cacheConfig</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaFuncCachePreferNone: no <span class="title function_">preference</span><span class="params">(<span class="keyword">default</span>)</span></span><br><span class="line">cudaFuncCachePreferShared: prefer 48KB shared memory and 16 KB L1 cache</span><br><span class="line">cudaFuncCachePreferL1: prefer 48KB L1 cache and 16 KB shared memory</span><br><span class="line">cudaFuncCachePreferEqual: prefer 32KB L1 cache and 32 KB shared memory</span><br></pre></td></tr></table></figure>

<p>通过不同核函数自动配置共享内存</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaFuncSetCacheConfig</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* func,<span class="keyword">enum</span> cudaFuncCacheca cheConfig)</span>;</span><br></pre></td></tr></table></figure>

<p>这里的func是核函数指针，当我们调用某个核函数时，次核函数已经配置了对应的L1和共享内存，那么其如果和当前配置不同，则会重新配置，否则直接执行。</p>
<h4 id="同步-1"><a href="#同步-1" class="headerlink" title="同步"></a>同步</h4><p>同步基本方法</p>
<ul>
<li>障碍：所有调用线程等待其余调用线程达到障碍点。</li>
<li>内存栅栏：所有调用线程必须等到全部内存修改对其余线程可见时才继续进行。</li>
</ul>
<p>弱排序内存</p>
<p>CUDA采用宽松的内存模型，也就是内存访问不一定按照他们在程序中出现的位置进行的。宽松的内存模型，导致了更激进的编译器。<strong>核函数内连续两个内存访问指令，如果独立，其不一定哪个先被执行。</strong></p>
<p>显示障碍</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure>

<p>内存栅栏</p>
<p>内存栅栏能保证栅栏前的内核内存写操作对栅栏后的其他线程都是可见的，有以下三种栅栏：块，网格，系统。</p>
<p>线程块内：保证同一块中的其他线程对于栅栏前的内存写操作可见</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_block();</span><br></pre></td></tr></table></figure>

<p>网格级内存栅栏：挂起调用线程，直到全局内存中所有写操作对相同的网格内的所有线程可见</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence();</span><br></pre></td></tr></table></figure>

<p>系统级栅栏：挂起调用线程，以保证该线程对全局内存，锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程可见。跨系统，<strong>包括主机和设备。</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_system();</span><br></pre></td></tr></table></figure>

<p>Volatile修饰符：olatile声明一个变量，防止编译器优化，防止这个变量存入缓存，<strong>始终存放于全局内存中。</strong></p>
<h4 id="行主序和列主序"><a href="#行主序和列主序" class="headerlink" title="行主序和列主序"></a>行主序和列主序</h4><ul>
<li><p><strong>访问</strong></p>
<p>当我们使用二维块的时候，可能会使用下列方式索引二维数组的数据</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">int</span> x[N][N];</span><br><span class="line">...</span><br><span class="line"><span class="type">int</span> a=X[thread.x][thread.y];</span><br></pre></td></tr></table></figure>

<p>由于共享内存按存储体进行存储，以<code>N=32</code>为例。这样访问就可能导致冲突最大化。如下图所示：</p>
<p><img src="/../images/image-20240925134203201.png" alt="image-20240925134203201"></p>
<p>我们的数据是按照行放进存储体中的这是固定的，所以我们希望，这个线程束中取数据是按照行来进行的，所以</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a=X[thread.y][thread.x];</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="常量内存-2"><a href="#常量内存-2" class="headerlink" title="常量内存"></a>常量内存</h3><p>读写权限： 对于内核代码是只读的，对于主机端是可读写的。</p>
<p>位置：DRAM，但在片上有对应的缓存。</p>
<p>最佳访问模式：线程束所有线程访问一个位置。</p>
<p>声明方式：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__constant 常量内存名</span><br></pre></td></tr></table></figure>

<p>生存周期：与应用程序生存周期相同，这就说明，<strong>所有网格（grid）对声明的常量内存都是可以访问的。</strong>运行时对主机可见，当CUDA独立编译被使用的，常量内存跨文件可见。</p>
<p>初始化常量内存：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpyToSymbol</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *symbol, <span class="type">const</span> <span class="type">void</span> * src,  <span class="type">size_t</span> count, <span class="type">size_t</span> offset, cudaMemcpyKind kind)</span></span><br></pre></td></tr></table></figure>

<p>用法类似<code>cudaMemcpy(d_a, h_a, nByte, cudaMemcpyHostToDevice)</code> </p>
<h3 id="只读缓存"><a href="#只读缓存" class="headerlink" title="只读缓存"></a>只读缓存</h3><p>特点：，只读缓存拥有从全局内存读取数据的专用带宽,所以，如果内核函数是带宽限制型的，那么这个帮助是非常大的，不同的设备有不同的只读缓存大小，Kepler SM有48KB的只读缓存，只读缓存对于分散访问的更好，当所有线程读取同一地址的时候常量缓存最好，只读缓存这时候效果并不好，只读换粗粒度为32。</p>
<p>使用方法：</p>
<ul>
<li><p>使用<code>__leg</code>函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">(<span class="type">float</span>* output, <span class="type">float</span>* input)</span> &#123;</span><br><span class="line">...</span><br><span class="line">output[idx] += __ldg(&amp;input[idx]);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用限定指针</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">kernel</span><span class="params">(<span class="type">float</span>* output, <span class="type">const</span> <span class="type">float</span>* __restrict__ input)</span> &#123;</span><br><span class="line">...</span><br><span class="line">output[idx] += input[idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>应用场景：<strong>常量缓存喜欢小数据，而只读内存加载的数据比较大。</strong></p>
<h3 id="线程束洗牌指令"><a href="#线程束洗牌指令" class="headerlink" title="线程束洗牌指令"></a>线程束洗牌指令</h3><h4 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h4><p>洗牌指令作用在线程束内，允许两个线程互相访问对方的寄存器，线程束内线程相互访问数据<strong>不通过共享内存或者全局内存</strong>，使得通信效率高很多，线程束洗牌指令传递数据，延迟极低，切不消耗内存。</p>
<h4 id="束内线程（Lane）"><a href="#束内线程（Lane）" class="headerlink" title="束内线程（Lane）"></a>束内线程（Lane）</h4><p>就是一个线程束内的索引，所以束内线程的ID在 【0,31】【0,31】 内，且唯一，唯一是指线程束内唯一。</p>
<h4 id="线程束洗牌指令的不同形式"><a href="#线程束洗牌指令的不同形式" class="headerlink" title="线程束洗牌指令的不同形式"></a>线程束洗牌指令的不同形式</h4><p>线程束内交换变量（整型和浮点型）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl(<span class="type">int</span> var,<span class="type">int</span> srcLane,<span class="type">int</span> width=warpSize);</span><br></pre></td></tr></table></figure>

<ul>
<li>var：返回的线程的<code>var</code>这个变量的值，对应的线程为<code>srcLane</code></li>
<li>srcLane：<code>srcLane</code>不<strong>是当前线程的束内线程</strong>，需要结合<code>width</code>算出来的相对线程。如我想得到3号线程内存的var值，而且width&#x3D;16，那么就是，0<del>15的束内线程接收0+3位置处的var值，也就是3号束内线程的var值，16</del>32的束内线程接收16+3&#x3D;19位置处的var变量。</li>
</ul>
<p><img src="/../images/image-20240920161956458.png" alt="image-20240920161956458"></p>
<p>从与调用线程相关的线程中复制数据</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_up(<span class="type">int</span> var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> with=warpSize);</span><br></pre></td></tr></table></figure>

<p>作用：调用线程得到当前束内线程编号减去delta的编号的线程内的var值。</p>
<p><img src="/../images/image-20240920162015488.png" alt="image-20240920162015488"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_down(<span class="type">int</span> var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> with=warpSize);</span><br></pre></td></tr></table></figure>

<p>作用：调用线程得到当前束内线程编号加上delta的编号的线程内的var值。</p>
<p><img src="/../images/image-20240920162227886.png" alt="image-20240920162227886"></p>
<p>异或线程数据</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_xor(<span class="type">int</span> var,<span class="type">int</span> laneMask,<span class="type">int</span> with=warpSize);</span><br></pre></td></tr></table></figure>

<p>作用：如果我们输入的laneMask是1，其对应的二进制是 000⋯001000⋯001 ,当前线程的索引是0~31之间的一个数，那么我们用laneMask与当前线程索引（操作当前线程索引的var值）进行抑或操作得到的就是目标线程的编号了。</p>
<p><img src="/../images/image-20240920162547354.png" alt="image-20240920162547354"></p>
<p>注意：shfl中计算目标线程编号的那步有取余操作，对with取余，我们真正得到的数据来自</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">srcLane=srcLane%width;</span><br></pre></td></tr></table></figure>

<h2 id="流和并发"><a href="#流和并发" class="headerlink" title="流和并发"></a>流和并发</h2><p>和线程这个概念不同，流的概念主要是从主机的角度取理解同步和异步的。</p>
<h3 id="并发和并行"><a href="#并发和并行" class="headerlink" title="并发和并行"></a>并发和并行</h3><p>并发：一个处理器同时处理多个任务。在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理机上运行。是<strong>互相抢占资源的</strong></p>
<p>并行：多个处理器或者多核的处理器同时处理多个任务。系统有一个以上CPU时，当一个CPU执行一个进程时，另一个CPU可以执行另一个进程，两个进程互不抢占CPU资源，可以同时进行，这种方式我们称之为并行(Parallel)。 这里面有一个很重要的点，那就是系统要有多个CPU才会出现并行。在有多个CPU的情况下，才会出现真正意义上的<strong>同时进行</strong>。<strong>不会出现互相抢占资源的情况。</strong></p>
<h3 id="同步和异步"><a href="#同步和异步" class="headerlink" title="同步和异步"></a>同步和异步</h3><p>同步：两个事物相互依赖，并且一个事物必须以依赖于另一事物的执行结果。比如在事物 <code>A-&gt;B</code> 事件模型中，你需要先完成事物 A 才能执行事物 B。也就是说，<strong>同步调用在被调用者未处理完请求之前，调用不返回，调用者会一直等待结果的返回。</strong> 同步请求就是要等待返回结果。</p>
<p>异步：两个事物完全独立，一个事物的执行不需要等待另外一个事物的执行。也就是说，异步调用<strong>可以返回结果不需要等待结果返回</strong>，当<strong>结果返回的时候通过回调函数或者其他方式带着调用结果再做相关事情</strong>。异步请求的时候不需要等待返回结果就去执行其他任务。</p>
<h3 id="阻塞和非阻塞"><a href="#阻塞和非阻塞" class="headerlink" title="阻塞和非阻塞"></a>阻塞和非阻塞</h3><p>阻塞：简单来说就是发出一个请求不能立刻返回响应，要等所有的逻辑全处理完才能返回响应。</p>
<p>非阻塞：发出一个请求立刻返回应答，不用等处理完所有逻辑。</p>
<p>阻塞与非阻塞指的是<strong>单个线程内</strong>遇到同步等待时，是否在原地不做任何操作。结合同步和异步，有以下几种分类。</p>
<ul>
<li><strong>同步阻塞</strong> 只有一个车道，不能超车，所有车子依次行使，一次只能通过一辆车，尴尬的是这个车道还堵车了。</li>
<li><strong>同步非阻塞</strong> 只有一个车道，不能超车，所有车子依次行使，一次只能通过一辆车，不过比较幸运这个车道没有堵车，可以正常通行。</li>
<li><strong>异步阻塞</strong> 有两个或两个以上车道，每条马路都可以通行，不同车道上的车子可以并行行使，尴尬的是所有的车道都堵车了。</li>
<li><strong>异步非阻塞</strong> 有两个或两个以上车道，每条马路都可以通行，不同车道上的车子可以并行行使，不过比较幸运的是没有一个车道堵车，都可以正常通行。</li>
</ul>
<h3 id="CUDA-流（Stream）"><a href="#CUDA-流（Stream）" class="headerlink" title="CUDA 流（Stream）"></a>CUDA 流（Stream）</h3><p>CUDA 流相当于把一系列CUDA操作封装起来。</p>
<p>一个 CUDA 流指的是由主机发出的在一个设备中执行的 CUDA 操作序列。除主机端发出的流之外，还有设备端发出的流。</p>
<p>以下主要讨论主机发出的流，一个 CUDA 流中的各个操作按照主机发布的次序执行；但来自两个不同 CUDA 流的操作不一定按照某个次序执行，有可能是并发或者交错地执行。<strong>流能封装异步操作，并保持操作顺序，允许操作在流中排队。保证其在前面所有操作启动之后启动。</strong></p>
<p>任何 CUDA 操作都存在于某个 CUDA 流中，如果没有明确指定 CUDA 流，那么所有 CUDA 操作都是在默认流中执行的。非默认 CUDA 流由<code>cudaStream_t</code>类型的变量表示。</p>
<p>为了产生多个相互独立的 CUDA 流、实现不同 CUDA 流之间的并发，主机在向某个 CUDA 流中发布命令后必须马上获取程序控制权，不等待该 CUDA 流中的命令在设备中执行完毕。</p>
<h4 id="默认流"><a href="#默认流" class="headerlink" title="默认流"></a><strong>默认流</strong></h4><p>属于隐式声明的流，也被叫做空流，是同步的。</p>
<p>在 CUDA 中，<strong>所有的设备操作都在流（stream）中执行</strong>。当没有指定流时，使用默认的流。</p>
<p>比如我们常见的套路，在主机端分配设备主存（cudaMalloc），主机向设备传输数据（cudaMemcpy），核函数启动，复制数据回主机（Memcpy），就属于默认的流。默认流是一个针对设备操作同步的流，也就是说，只有当所有之前设备上任何流里面的操作全部完成时，才开始默认流里面操作的执行，并且默认流里面的一个操作必须完成，其他任何流里面的操作才能开始。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(d_a, a, numBytes, cudaMemcpyHostToDevice); </span><br><span class="line">increment&lt;&lt;&lt;<span class="number">1</span>,N&gt;&gt;&gt;(d_a);</span><br><span class="line">cudaMemcpy(a, d_a, numBytes, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>

<p>从设备端来看，这三个操作都在默认流中，并且按顺序执行；第一步主机到设备的数据传输是同步的，CPU线程不能到达第二行直到主机到设备的数据传输完成。 kernel 是异步的，即主机发出调用核函数的命令后，不会等待命令执行完毕，而会立刻取得程序控制权，然后紧接着发出最后一个<code>cudaMemcpy</code>命令，CPU线程移到第三行但是该命令不会立即被执行，因为这是默认流中的CUDA操作，必须等待前一个CUDA操作（即核函数的调用）执行完毕才会开始执行。</p>
<h4 id="非默认流"><a href="#非默认流" class="headerlink" title="非默认流"></a><strong>非默认流</strong></h4><p>属于显式声明的流，也被叫做非空流，是异步的。</p>
<p><strong>非默认流的创建和销毁</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaStreamCreate</span><span class="params">(cudaStream_t* pStream)</span>;</span><br><span class="line">cudaError_t <span class="title function_">cudaStreamDestroy</span><span class="params">(cudaStream_t stream)</span>;</span><br></pre></td></tr></table></figure>

<p>对于回收函数，由于流和主机端是异步的，你在使用上面指令回收流的资源的时候，很有可能流还在执行，这时候，这条指令会正常执行，但是不会立刻停止流，而是等待流执行完成后，立刻回收该流中的资源。这样做是合理的也是安全的。</p>
<p><strong>检查流的操作是否在设备中完成</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//阻塞主机直到stream中的所有操作都执行完毕</span></span><br><span class="line">cudaError_t <span class="title function_">cudaStreamSynchronize</span><span class="params">(cudaStream_t stream)</span>;</span><br><span class="line"><span class="comment">//不阻塞主机，只检查stream中的所有操作是否都执行完毕，若是则返回cudaSuccess，否则返回cudaErrorNotReady</span></span><br><span class="line">cudaError_t <span class="title function_">cudaStreamQuery</span><span class="params">(cudaStream_t stream)</span>;</span><br></pre></td></tr></table></figure>

<p><strong>异步的数据传输</strong></p>
<p>意味着数据传输指令执行之后，控制权会立刻交给主机，不需要等待数据传输指令执行完毕。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpyAsync</span><span class="params">(<span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count,cudaMemcpyKind kind, cudaStream_t stream = <span class="number">0</span>)</span>;</span><br></pre></td></tr></table></figure>

<p><strong>执行异步数据传输时，主机端的内存必须是固定的，非分页的！</strong></p>
<p>在非空流中执行内核需要在启动核函数的时候加入一个附加的启动配置：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid, block, sharedMemSize, stream&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure>

<p>pStream参数就是附加的参数，使用目标流的名字作为参数，比如想把核函数加入到a流中，那么这个stream就变成a。</p>
<p>举个栗子：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * bytesPerStream;</span><br><span class="line">    cudaMemcpyAsync(&amp;d_a[offset], &amp;a[offset], bytePerStream, streams[i]);</span><br><span class="line">    kernel&lt;&lt;grid, block, <span class="number">0</span>, streams[i]&gt;&gt;(&amp;d_a[offset]);</span><br><span class="line">    cudaMemcpyAsync(&amp;a[offset], &amp;d_a[offset], bytesPerStream, streams[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">    cudaStreamSynchronize(streams[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>第一个for中循环执行了nStreams个流，每个流中都是“复制数据，执行核函数，最后将结果复制回主机”这一系列操作。<br>下面的图就是一个简单的时间轴示意图，假设nStreams&#x3D;3，所有传输和核启动都是并发的：</p>
<p><img src="/../images/image-20240921162536356.png" alt="image-20240921162536356"></p>
<p>H2D是主机到设备的内存传输，D2H是设备到主机的内存传输。显然这些操作没有并发执行，而是错开的，原因是PCIe总线是共享的，当第一个流占据了主线，后来的就一定要等待，等待主线空闲。编程模型和硬件的实际执行时有差距了。</p>
<p>上面同时从主机到设备涉及硬件竞争要等待，<strong>如果是从主机到设备和从设备到主机同时发生，这时候不会产生等待，而是同时进行。</strong></p>
<h3 id="流调度"><a href="#流调度" class="headerlink" title="流调度"></a>流调度</h3><p>从编程模型看，所有流可以同时执行，但是硬件毕竟有限，不可能像理想情况下的所有流都有硬件可以使用，所以硬件上如何调度这些流是我们理解流并发的关键。</p>
<h4 id="虚假的依赖关系"><a href="#虚假的依赖关系" class="headerlink" title="虚假的依赖关系"></a><strong>虚假的依赖关系</strong></h4><p>Fermi架构上16路流并发执行但是所有流最终都是在单一硬件上执行的，Fermi只有一个硬件工作队列，所以他们虽然在编程模型上式并行的，但是在硬件执行过程中是在一个队列中（像串行一样）。要执行某个网格的时候CUDA会检测任务依赖关系，如果其依赖于其他结果，那么要等结果出来后才能继续执行。单一流水线可能会导致虚假依赖关系：</p>
<p><img src="/../images/image-20240921163332876.png" alt="image-20240921163332876"></p>
<ol>
<li>执行A，同时检查B是否有依赖关系，当然此时B依赖于A而A没执行完，所以整个队列阻塞</li>
<li>A执行完成后执行B，同时检查C，发现依赖，等待</li>
<li>B执行完后，执行C同时检查，发现P没有依赖，如果此时硬件有多于资源P开始执行</li>
<li>P执行时检查Q，发现Q依赖P，所以等待</li>
</ol>
<p>种一个队列的模式，会产生一种，虽然P依赖B的感觉，虽然不依赖，但是B不执行完，P没办法执行，而所谓并行，只有一个依赖链的头和尾有可能并行，也就是红圈中任务可能并行，而我们的编程模型中设想的并不是这样的。</p>
<h4 id="Hyper-Q技术-1"><a href="#Hyper-Q技术-1" class="headerlink" title="Hyper-Q技术"></a><strong>Hyper-Q技术</strong></h4><p>使用多个工作队列，解决了虚假的依赖关系。</p>
<p><img src="/../images/image-20240921163623545.png" alt="image-20240921163623545"></p>
<h3 id="流的优先级"><a href="#流的优先级" class="headerlink" title="流的优先级"></a>流的优先级</h3><p>优先级<strong>只影响核函数，不影响数据传输</strong>，高优先级的流可以占用低优先级的工作。</p>
<h4 id="创建有指定优先级的流"><a href="#创建有指定优先级的流" class="headerlink" title="创建有指定优先级的流"></a><strong>创建有指定优先级的流</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaStreamCreateWithPriority</span><span class="params">(cudaStream_t* pStream, <span class="type">unsigned</span> <span class="type">int</span> flags,<span class="type">int</span> priority)</span>;</span><br></pre></td></tr></table></figure>

<p>参数说明</p>
<ul>
<li><code>pStream</code>: 流指针</li>
<li><code>flags</code>: 决定流是阻塞流还是非阻塞流。<code>cudaStreamDefault</code>：默认阻塞流。<code>cudaStreamNonBlocking</code>：非阻塞流。即可以和空流同时进行，对空流的阻塞行为失效。</li>
<li><code>priority</code>：优先级别。越接近0优先级越高。</li>
</ul>
<h4 id="查询当前设备的优先级分布情况"><a href="#查询当前设备的优先级分布情况" class="headerlink" title="查询当前设备的优先级分布情况"></a><strong>查询当前设备的优先级分布情况</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceGetStreamPriorityRange</span><span class="params">(<span class="type">int</span> *leastPriority, <span class="type">int</span> *greatestPriority)</span>;</span><br></pre></td></tr></table></figure>

<p><code>leastPriority</code>表示最低优先级（整数，远离0）<br><code>greatestPriority</code>表示最高优先级（整数，数字较接近0）<br>如果设备不支持优先级返回0。</p>
<h3 id="CUDA-事件（Event）"><a href="#CUDA-事件（Event）" class="headerlink" title="CUDA 事件（Event）"></a>CUDA 事件（Event）</h3><p>事件的本质就是一个标记，可以使用事件来执行以下两个基本任务</p>
<ul>
<li>同步流执行</li>
<li>监控设备进展</li>
</ul>
<p>流中的任意点都可以通过API插入事件以及查询事件完成的函数<strong>，只有事件所在流中其之前的操作都完成后</strong>才能触发事件完成。</p>
<p>事件就像一个个路标，其本身不执行什么功能，就像我们最原始测试c语言程序的时候插入的无数多个<code>printf</code>一样。</p>
<p><strong>事件声明</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t event;</span><br></pre></td></tr></table></figure>

<p><strong>创建事件</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventCreate</span><span class="params">(cudaEvent_t* event)</span>;</span><br></pre></td></tr></table></figure>

<p><strong>销毁事件</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventDestroy</span><span class="params">(cudaEvent_t event)</span>;</span><br></pre></td></tr></table></figure>

<p>与流的销毁类似，如果回收指令执行的时候事件还没有完成，那么回收指令立即完成，当事件完成后，资源马上被回收。</p>
<p><strong>将事件插入流中</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventRecord</span><span class="params">(cudaEvent_t event, cudaStream_t stream = <span class="number">0</span>)</span>;  </span><br></pre></td></tr></table></figure>

<p><strong>检查事件的操作是否在设备中完成</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 阻塞主机线程直到事件被完成</span></span><br><span class="line">cudaError_t <span class="title function_">cudaEventSynchronize</span><span class="params">(cudaEvent_t event)</span>;</span><br><span class="line"><span class="comment">// 不阻塞主机，只检查事件中的所有操作是否都执行完毕，若是则返回cudaSuccess，否则返回cudaErrorNotReady</span></span><br><span class="line">cudaError_t <span class="title function_">cudaEventQuery</span><span class="params">(cudaEvent_t event)</span>;</span><br></pre></td></tr></table></figure>

<p><strong>记录两个事件的时间间隔</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventElapsedTime</span><span class="params">(<span class="type">float</span>* ms, cudaEvent_t start, cudaEvent_t stop)</span>;</span><br></pre></td></tr></table></figure>

<p>这个函数记录两个事件<code>start</code>和<code>stop</code>之间的时间间隔，单位毫秒，两个事件不一定是同一个流中。这个时间间隔可能会比实际大一些，因为<code>cudaEventRecord</code>这个函数是异步的，所以加入时间完全不可控，不能保证两个事件之间的间隔刚好是两个事件之间的。</p>
<h3 id="流同步"><a href="#流同步" class="headerlink" title="流同步"></a>流同步</h3><p>cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成。举个栗子：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream_1;</span><br><span class="line">cudaStream_t stream_2;</span><br><span class="line">cudaError_t <span class="title function_">cudaStreamCreate</span><span class="params">(cudaStream_t* stream_1)</span>;</span><br><span class="line">cudaError_t <span class="title function_">cudaStreamCreate</span><span class="params">(cudaStream_t* stream_2)</span>;</span><br><span class="line">kernel_1&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream_1&gt;&gt;&gt;();</span><br><span class="line">kernel_2&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">kernel_3&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream_2&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure>

<p>上面这段代码，有三个流，两个有名字的，一个空流，我们认为stream_1和stream_2是阻塞流，空流是阻塞的，这三个核函数都在阻塞流上执行，具体过程是，kernel_1被启动，控制权返回主机，然后启动kernel_2，但是此时kernel_2 不会并不会马山执行，他会等到kernel_1执行完毕，同理启动完kernel_2 控制权立刻返回给主机，主机继续启动kernel_3,这时候kernel_3 也要等待，直到kernel_2执行完，但是从主机的角度，这三个核都是异步的，启动后控制权马上还给主机。</p>
<h4 id="创建非阻塞流"><a href="#创建非阻塞流" class="headerlink" title="创建非阻塞流"></a><strong>创建非阻塞流</strong></h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaStreamCreateWithFlags</span><span class="params">(cudaStream_t* pStream, <span class="type">unsigned</span> <span class="type">int</span> flags)</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>flags</code>：决定流是阻塞流还是非阻塞流。<code>cudaStreamDefault</code>：默认阻塞流。<code>cudaStreamNonBlocking</code>：非阻塞流。即可以和空流同时进行，对空流的阻塞行为失效。</li>
</ul>
<p>如果前面的stream_1和stream_2声明为非阻塞的，那么上面的调用方法的结果是三个核函数同时执行。</p>
<h4 id="隐式同步"><a href="#隐式同步" class="headerlink" title="隐式同步"></a><strong>隐式同步</strong></h4><p>所谓同步就是阻塞的意思，被忽视的隐式同步就是被忽略的阻塞，隐式操作常出现在内存操作上，比如：</p>
<ul>
<li>锁页主机内存分布</li>
<li>设备内存分配</li>
<li>设备内存初始化</li>
<li>同一设备两地址之间的内存复制</li>
<li>一级缓存，共享内存配置修改</li>
</ul>
<h4 id="显式同步"><a href="#显式同步" class="headerlink" title="显式同步"></a><strong>显式同步</strong></h4><p>常见显式同步</p>
<ul>
<li>同步设备</li>
<li>同步流</li>
<li>同步流中的事件</li>
<li>使用事件跨流同步</li>
</ul>
<p>显式同步常用函数:</p>
<ul>
<li><p>阻塞主机线程，直到设备完成所有操作</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceSynchronize</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>同步流，阻塞主机直到完成</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaStreamSynchronize</span><span class="params">(cudaStream_t stream)</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试一下流是否完成</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventQuery</span><span class="params">(cudaEvent_t event)</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>同步事件，阻塞主机直到完成</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventSynchronize</span><span class="params">(cudaEvent_t event)</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试一下事件是否完成</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventQuery</span><span class="params">(cudaEvent_t event)</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>流之间同步</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaStreamWaitEvent</span><span class="params">(cudaStream_t stream, cudaEvent_t event)</span>;</span><br></pre></td></tr></table></figure>

<p>指定的流要等待指定的事件，事件完成后流才能继续，这个事件可以在这个流中，也可以不在，当在不同的流的时候，这个就是实现了跨流同步。</p>
</li>
</ul>
<h3 id="可配置事件"><a href="#可配置事件" class="headerlink" title="可配置事件"></a>可配置事件</h3><p>控制事件行为和性能</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaEventCreateWithFlags</span><span class="params">(cudaEvent_t* event, <span class="type">unsigned</span> <span class="type">int</span> flags)</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>flags</code>：<code>cudaEventDefault</code>、<code>cudaEventBlockingSync</code>、<code>cudaEventDisableTiming</code>、<code>cudaEventInterprocess</code>。</li>
</ul>
<p>其中cudaEventBlockingSync指定使用cudaEventSynchronize同步会造成阻塞调用线程。cudaEventSynchronize默认是使用cpu周期不断重复查询事件状态，而当指定了事件是cudaEventBlockingSync的时候，会将查询放在另一个线程中，而原始线程继续执行，直到事件满足条件，才会通知原始线程，这样可以减少CPU的浪费，但是由于通讯的时间，会造成一定的延迟。<br>cudaEventDisableTiming表示事件不用于计时，可以减少系统不必要的开支也能提升cudaStreamWaitEvent和cudaEventQuery的效率<br>cudaEventInterprocess表明可能被用于进程之间的事件。</p>
<h3 id="创建流间依赖关系"><a href="#创建流间依赖关系" class="headerlink" title="创建流间依赖关系"></a>创建流间依赖关系</h3><p>流之间的虚假依赖关系是需要避免的，而经过我们设计的依赖又可以保证流之间的同步性，避免内存竞争，这时候我们要使用的就是事件这个工具了，换句话说，我们可以让某个特定流等待某个特定的事件，这个事件可以再任何流中，只有此事件完成才能进一步执行等待此事件的流继续执行。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t * event=(cudaEvent_t *)<span class="built_in">malloc</span>(n_stream*<span class="keyword">sizeof</span>(cudaEvent_t));</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++)</span><br><span class="line">&#123;</span><br><span class="line">    cudaEventCreateWithFlag(&amp;event[i],cudaEventDisableTiming); <span class="comment">//这种事件往往不用于计时，所以可以在生命的时候声明成 cudaEventDisableTiming 的同步事件</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++)</span><br><span class="line">&#123;</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_3&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    cudaEventRecord(event[i],stream[i]);</span><br><span class="line">    cudaStreamWaitEvent(stream[n_stream<span class="number">-1</span>],event[i],<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这时候，最后一个流（第5个流）都会等到前面所有流中的事件完成，自己才会完成。</p>
<h3 id="流回调"><a href="#流回调" class="headerlink" title="流回调"></a>流回调</h3><p>流回调是一种特别的技术，有点像是事件的函数，这个回调函数被放入流中，当其前面的任务都完成了，就会调用这个函数，但是比较特殊的是，在回调函数中，需要遵守下面的规则。</p>
<ul>
<li>回调函数中不可以调用CUDA的API</li>
<li>不可以执行同步</li>
</ul>
<p><strong>流函数</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> CUDART_CB <span class="title function_">my_callback</span><span class="params">(cudaStream_t stream, cudaError_t status, <span class="type">void</span> *data)</span> &#123;</span><br><span class="line">   ....;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>将流函数加入流中</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaStreamAddCallback</span><span class="params">(cudaStream_t stream,cudaStreamCallback_t callback, <span class="type">void</span> *userData, <span class="type">unsigned</span> <span class="type">int</span> flags)</span>;</span><br></pre></td></tr></table></figure>



<h1 id="CUDA常见函数"><a href="#CUDA常见函数" class="headerlink" title="CUDA常见函数"></a><strong>CUDA常见函数</strong></h1><h2 id="cudasetdevice-n"><a href="#cudasetdevice-n" class="headerlink" title="cudasetdevice(n)"></a><code>cudasetdevice(n)</code></h2><p>cudaSetDevice函数用来设置要在哪个GPU上执行，如果只有一个GPU，设置为cudaSetDevice（0）</p>
<h2 id="cudaDeviceReset"><a href="#cudaDeviceReset" class="headerlink" title="cudaDeviceReset()"></a><code>cudaDeviceReset()</code></h2><p>调用核函数时，这句话如果没有，则不能正常的运行，因为这句话包含了隐式同步，GPU和CPU执行程序是异步的，核函数调用后成立刻会到主机线程继续，而不管GPU端核函数是否执行完毕，所以上面的程序就是GPU刚开始执行，CPU已经退出程序了，所以我们要等GPU执行完了，再退出主机线程。</p>
<h2 id="cudaDeviceSynchronize"><a href="#cudaDeviceSynchronize" class="headerlink" title="cudaDeviceSynchronize()"></a><code>cudaDeviceSynchronize()</code></h2><p>想要主机等待设备端执行可以用下面这个指令。</p>
<h2 id="cudaMalloc-void-devPtr-size-t-nByte"><a href="#cudaMalloc-void-devPtr-size-t-nByte" class="headerlink" title="cudaMalloc(void **devPtr, size_t nByte)"></a><code>cudaMalloc(void **devPtr, size_t nByte)</code></h2><p>cuda中申请分配显存的函数，注意传入参数，第一个传入参数是一个双指针类型，第二个是一共申请的内存大小（字节数），重点理解第一个传入参数。示例如下。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *device_data = <span class="literal">NULL</span>;</span><br><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span>*<span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;device_data,size); </span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;device_data,size)); <span class="comment">//和CHECK联合使用，在log中检查错误类型。</span></span><br></pre></td></tr></table></figure>

<p>第一行在主机端声明了一个指针，指向我们需要操作的数据。第二行计算需要申请多少字节数。第三行<code>&amp;device_data</code>传入的是声明指针的地址，也就是指针变量的地址，再将其强制类型转换成指针的指针，这样可以在函数内部改变<code>**</code>中指向的<code>*</code>的值，这个值也即是<code>device_data</code>这个指针变量，而不是它指向的值。此时我们可以将<strong>显存中申请的数组的首地址</strong>赋值给<code>device_data</code>(不是它指向的值)，就可以完成内存申请了。</p>
<h2 id="cudaMemcpy-d-a-h-a-nByte-cudaMemcpyHostToDevice"><a href="#cudaMemcpy-d-a-h-a-nByte-cudaMemcpyHostToDevice" class="headerlink" title="cudaMemcpy(d_a, h_a, nByte, cudaMemcpyHostToDevice)"></a><code>cudaMemcpy(d_a, h_a, nByte, cudaMemcpyHostToDevice)</code></h2><p>函数作用：实现主机和设备的内存复制。<strong>是同步的</strong>，需要执行完毕后才能进行下一步。</p>
<p>参数含义：复制的目标，复制的源头，复制的字节大小，复制的类型，分别有<code>cudaMemcpyHostToHost</code>，<code>cudaMemcpyHostToDevice</code>，<code>cudaMemcpyDeviceToHost</code>，<code>cudaMemcpyDeviceToDevice</code>,代表内存复制的方向。</p>
<h2 id="cudaMemcpyAsync-void-dst-const-void-src-size-t-count-cudaMemcpyKind-kind-cudaStream-t-stream-0"><a href="#cudaMemcpyAsync-void-dst-const-void-src-size-t-count-cudaMemcpyKind-kind-cudaStream-t-stream-0" class="headerlink" title="cudaMemcpyAsync(void* dst, const void* src, size_t count,cudaMemcpyKind kind, cudaStream_t stream = 0)"></a><code>cudaMemcpyAsync(void* dst, const void* src, size_t count,cudaMemcpyKind kind, cudaStream_t stream = 0)</code></h2><p>函数作用：与<code>cudaMemcpy</code>相似，<strong>只不过是异步的。</strong>cudaMemcpyAsync发出命令后主机就不等待了。</p>
<p>参数含义：复制的目标，复制的源头，复制的字节大小，复制的类型，分别有<code>cudaMemcpyHostToHost</code>，<code>cudaMemcpyHostToDevice</code>，<code>cudaMemcpyDeviceToHost</code>，<code>cudaMemcpyDeviceToDevice</code>,代表内存复制的方向。<code>stream</code>表示流，一般情况设置为默认流。</p>
<h2 id="cudaMemcpyToSymbol-const-void-symbol-const-void-src-size-t-count-size-t-offset-cudaMemcpyKind-kind"><a href="#cudaMemcpyToSymbol-const-void-symbol-const-void-src-size-t-count-size-t-offset-cudaMemcpyKind-kind" class="headerlink" title="cudaMemcpyToSymbol(const void *symbol, const void * src,  size_t count, size_t offset, cudaMemcpyKind kind)"></a><code>cudaMemcpyToSymbol(const void *symbol, const void * src,  size_t count, size_t offset, cudaMemcpyKind kind)</code></h2><p>函数作用：把主机上申请的常量内存复制到设备的常量内存中。</p>
<p>参数含义：复制的目标、源主机地址、复制的字节数、偏移量（不设置默认为0）、传输方向（一般为<code>cudaMemcpyHostToDevice</code>）</p>
<h2 id="cudaFuncSetCacheConfig-const-void-func-enum-cudaFuncCache"><a href="#cudaFuncSetCacheConfig-const-void-func-enum-cudaFuncCache" class="headerlink" title="cudaFuncSetCacheConfig(const void * func,enum cudaFuncCache)"></a><span id="SetCacheConfig"><code>cudaFuncSetCacheConfig(const void * func,enum cudaFuncCache)</code></span></h2><p>函数作用：设置SM中一级缓存和共享内存共享的片上内存。这个函数可以设置内核的共享内存和一级缓存之间的比例。</p>
<p>参数设置：cudaFuncCache参数可选如下配置</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaFuncCachePreferNone<span class="comment">//无参考值，默认设置</span></span><br><span class="line">cudaFuncCachePreferShared<span class="comment">//48k共享内存，16k一级缓存</span></span><br><span class="line">cudaFuncCachePreferL1<span class="comment">// 48k一级缓存，16k共享内存</span></span><br><span class="line">cudaFuncCachePreferEqual<span class="comment">// 32k一级缓存，32k共享内存</span></span><br></pre></td></tr></table></figure>

<h2 id="cudaMemcpyToSymbol-const-void-symbol-const-void-src-size-t-count"><a href="#cudaMemcpyToSymbol-const-void-symbol-const-void-src-size-t-count" class="headerlink" title="cudaMemcpyToSymbol(const void* symbol,const void *src,size_t count)"></a><span id="cudaMemcpyToSymbol"><code>cudaMemcpyToSymbol(const void* symbol,const void *src,size_t count)</code></span></h2><p>函数作用：初始化常量内存，从src复制count个字节的内存到symbol里面，也就是设备端的常量内存，也可以将内存拷贝到全局内存中。</p>
<p>参数含义：<code>symbol</code>设备端常量内存，<code>src</code>主机端内存，<code>count</code>复制的字节数。</p>
<p>注：这个函数是同步的，会马上被执行。</p>
<h2 id="cudaMemcpyFromSymbol-const-void-symbol-const-void-src-size-t-count"><a href="#cudaMemcpyFromSymbol-const-void-symbol-const-void-src-size-t-count" class="headerlink" title="cudaMemcpyFromSymbol(const void* symbol,const void *src,size_t count)"></a><code>cudaMemcpyFromSymbol(const void* symbol,const void *src,size_t count)</code></h2><p>函数作用：从symbol复制count个字节的内存到src里面，也就是将设备端的变量复制到主机端。</p>
<p>参数含义：<code>symbol</code>设备端常量内存，<code>src</code>主机端内存，<code>count</code>复制的字节数。</p>
<h2 id="cudaGetSymbolAddress-void-dptr-devData"><a href="#cudaGetSymbolAddress-void-dptr-devData" class="headerlink" title="cudaGetSymbolAddress((void**)&amp;dptr,devData)"></a><code>cudaGetSymbolAddress((void**)&amp;dptr,devData)</code></h2><p>函数作用：获取设备端变量devData的地址。</p>
<p>参数含义：<code>dptr</code>设备端变量地址，这个变量是指针的指针<code>devData</code>设备端变量。</p>
<h2 id="cudaError-t-cudaMemset-void-devPtr-int-value-size-t-count"><a href="#cudaError-t-cudaMemset-void-devPtr-int-value-size-t-count" class="headerlink" title="cudaError_t cudaMemset(void *devPtr,int value, size_t count)"></a><code>cudaError_t cudaMemset(void *devPtr,int value, size_t count)</code></h2><p>函数作用：这一段内存的值都分配为value</p>
<p>参数含义：<code>devPtr</code>要被分配的地址，<code>value</code>被赋值的值，<code>count</code>字节数</p>
<h1 id="CUDA工具"><a href="#CUDA工具" class="headerlink" title="CUDA工具"></a>CUDA工具</h1><h2 id="Nsight"><a href="#Nsight" class="headerlink" title="Nsight"></a>Nsight</h2><h3 id="Nsight-Compute-1"><a href="#Nsight-Compute-1" class="headerlink" title="Nsight Compute"></a>Nsight Compute</h3><p><code>Nsight Compute</code>通过用户界面和命令行工具提供详细的性能指标和API调试。此外，它的基线特性允许用户在工具中比较结果。NVIDIA Nsight Compute提供了一个可定制的、数据驱动的用户界面和度量集合，并且可以通过分析脚本对后处理结果进行扩展。</p>
<h4 id="GUI模式"><a href="#GUI模式" class="headerlink" title="GUI模式"></a>GUI模式</h4><ul>
<li><p>使用时要以管理员权限运行，否则会出现GPU访问权限的问题</p>
<p><img src="/../images/image-20240923162806399.png" alt="image-20240923162806399"></p>
</li>
<li><p>连接页面选择连接的设备ip和可执行文件的位置</p>
<p><img src="/../images/image-20240923163215192.png"></p>
</li>
<li><p>点击<code>Run to next kernel</code></p>
<p><img src="/../images/image-20240923163336844.png" alt="image-20240923163336844"></p>
</li>
<li><p>此时程序暂停到核函数前，然后点击<code>Profile Kernel</code>获取详细信息</p>
<p><img src="/../images/image-20240923163540081.png" alt="image-20240923163540081"></p>
<p>注：只有程序暂停到核函数前，<code>Profile Kernel</code>才可以点击。</p>
</li>
<li><p>点击<code>Page</code>的不同选项，可以得到对应信息</p>
<p><img src="/../images/image-20240923163745596.png" alt="image-20240923163745596"></p>
</li>
</ul>
<h4 id="CLI模式"><a href="#CLI模式" class="headerlink" title="CLI模式"></a>CLI模式</h4><p>使用前需要配置好环境变量</p>
<p>基本使用：要收集目标应用程序中所有内核启动的“基本”集，在CUDA可执行文件目录下，以管理员权限打开该目录，输入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ncu -o profile xx.exe</span><br></pre></td></tr></table></figure>

<p>常用命令</p>
<ul>
<li><p><strong>I&#x2F;O</strong></p>
<p><code>-o</code> 参数：指定输出的分析报告<code>.ncu-rep</code> 的路径以及名称；</p>
<p><code>-f</code>参数：如果ncu指定了<code>-o</code> 参数、输出分析报告，分析报告指定的路径下有同名文件，可以指定<code>-f</code>参数强制将同名文件覆盖，否则ncu会报错停止运行；</p>
<p><code>—-log-file</code> 参数：将ncu在运行时产生的log存在指定路径的文件中；</p>
<p>如果在ncu运行时不指定<code>-o</code> 参数，则不会产生分析报告，并会将所有的分析结果打印在终端中，同样可以通过设置参数来决定终端中显示什么样的信息：</p>
<p><code>—-page</code>参数：选择在终端中打印分析报告中的哪部分信息，一般使用<code>details</code> 来打印用户指定采集的分析指标的详细信息；</p>
<p><code>—-csv</code>参数：将终端中打印的Kernel的分析信息组织成csv格式的输出，方便导出到文件中进行后续的数据分析。</p>
<p>ncu还可以使用<code>—-import</code> 将之前生成的分析报告读入，然后根据<code>—-page</code> 、<code>—-csv</code> 等参数的设置将分析报告在终端中打印出来。</p>
</li>
<li><p><strong>Filter</strong></p>
<p>ncu在运行时可以根据参数的设置对要profiling的<strong>Kernel、进程、设备</strong>等进行过滤，首先来看Kernel的过滤：</p>
<p><code>—-kernel-name</code> 参数：支持根据准确名称或正则表达式来过滤需要进行profiling的Kernel</p>
<p><code>-c</code> 参数：限制程序中进行profiling的Kernel函数的总数</p>
<p><code>-s</code> 参数：跳过程序中的前N个Kernel后再进行profiling</p>
<p>再来看一些ncu对于需要进行profiling的进程进行选择的参数：</p>
<p><code>—-target-processes</code> 参数：选择ncu目标进行profiling的进程，<code>application-only</code> 选项只会让ncu对根进程进行profiling（然而这些根进程一般都只是启用后续的程序用的，捕捉不到什么有用的Kernel信息），<code>all</code> 选项会对根程序产生的所有子进程进行profiling，一般来说我们选择这个选项。</p>
<p><code>—-target-processes-filter</code> 参数：支持通过准确名称或正则表达式来过滤ncu需要进行profiling的进程，具体可以参见Nvidia文档中的说明。</p>
<p>最后是ncu对于需要进行profiling的设备进行选择的参数：</p>
<p><code>—devices</code> 参数：指定需要使用ncu进行profiling的（GPU）设备编号（默认是所有参与的GPU），<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/option-to-profile-only-master-process/264727/2?u=jiaqi-ruan">Nvidia官方推荐在每个Node中只使用一个设备进行profiling</a>，否则可能会引发程序的stall</p>
</li>
<li><p><strong>Profile Section</strong></p>
<p>使用ncu进行profiling时可以获取关于Kernel各个方面的指标，但是如果运行时需要获取所有的指标的话就可能导致ncu运行的时间非常长，如果只想获取到特定方面的指标，可以通过参数<code>—-section</code> 指定在profiling时需要采集的特定方面的指标，这样就可以有效减少不必要的profiling时间，关于所有的section的选项可以在<a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sections-and-rules">Nvida官方文档</a>中进行查看，这里列举一些常用的section进行说明：</p>
<p><code>MemoryWorkloadAnalysis</code>： 可以让ncu对Kernel在内存利用方面的指标进行profiling，具体包含最大吞吐率、最大带宽利用率等指标；</p>
<p><code>ComputeWorkloadAnalysis</code>：Detailed analysis of the compute resources of the streaming multiprocessors (SM), including the achieved instructions per clock (IPC) and the utilization of each available pipeline. Pipelines with very high utilization might limit the overall performance.</p>
<p>指标示例：</p>
<p><a target="_blank" rel="noopener" href="https://img.androsheep.win/blog/CWA_example.png"><img src="https://img.androsheep.win/blog/CWA_example.png" alt="CWA example"></a></p>
<p><code>Occupancy</code>：Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. Another way to view occupancy is the percentage of the hardware’s ability to process warps that is actively in use. Higher occupancy does not always result in higher performance, however, low occupancy always reduces the ability to hide latencies, resulting in overall performance degradation. Large discrepancies between the theoretical and the achieved occupancy during execution typically indicates highly imbalanced workloads.</p>
<p><img src="/../images/Occupancy_example.png" alt="img"></p>
</li>
</ul>
<h3 id="Nsight-System"><a href="#Nsight-System" class="headerlink" title="Nsight System"></a>Nsight System</h3><p>所有与NVIDIA GPU相关的程序开发都可以从Nsight System开始以确定最大的优化机会。Nsight System给开发者一个系统级别的应用程序性能的可视化分析。开发人员可以优化瓶颈，以便在任意数量或大小的CPU和GPU之间实现高效扩展</p>
<p>用户手册地址：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">User Guide — nsight-systems 2024.5 documentation (nvidia.com)</a></p>
<h4 id="GUI模式-1"><a href="#GUI模式-1" class="headerlink" title="GUI模式"></a>GUI模式</h4><p>与Nsight Compute类似，指定好可执行文件目录后，点击开始即可。</p>
<p><img src="/../images/image-20240923193742385.png" alt="image-20240923193742385"></p>
<h4 id="CLI模式-1"><a href="#CLI模式-1" class="headerlink" title="CLI模式"></a>CLI模式</h4><p>必须以管理员身份在 Windows 上运行 CLI。</p>
<ul>
<li><p>命令行选项</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys [global_option]</span><br></pre></td></tr></table></figure>

<p>or</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys [command_switch][optional command_switch_options][application] [optional application_options]</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成分析结果文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys profile –t cuda,osrt,nvtx –o baseline –w true python main.py</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-t</code>后面跟定的参数是我们要追踪的API，即需要CUDA API，OS runtime API以及NVTX API</li>
<li><code>-o</code>给定的是输出的文件名称</li>
<li><code>-w </code>后面表明是或否要在命令行中同时输出结果</li>
<li><code>python main.py</code>为程序的执行命令</li>
<li>将导出的baseline输出文件下载到本地，并拖拽到本地的Nsight System窗口即可获取性能结果展示。</li>
</ul>
</li>
</ul>
<h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40653140/article/details/136238420">CUDA编程 - Nsight system &amp; Nsight compute 的安装和使用 - 学习记录-CSDN博客</a></p>
<h2 id="NVIDIA-Compute-Sanitizer"><a href="#NVIDIA-Compute-Sanitizer" class="headerlink" title="NVIDIA Compute Sanitizer"></a>NVIDIA Compute Sanitizer</h2><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h3 id="Q1-CUDA编程中引入头文件和链接器的问题"><a href="#Q1-CUDA编程中引入头文件和链接器的问题" class="headerlink" title="Q1 CUDA编程中引入头文件和链接器的问题"></a>Q1 CUDA编程中引入头文件和链接器的问题</h3><p><strong>要点1：</strong> 在项目属性中添加CUDA的<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=bin%E6%96%87%E4%BB%B6&spm=1001.2101.3001.7020">bin文件</a></p>
<p><strong>要点2：</strong> 在项目属性中添加CUDA的lib文件</p>
<p><strong>要点3</strong>：自定义生成依赖项（此处不要忘记）</p>
<p>具体步骤参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45241855/article/details/129873676">vs2019 在C++项目中添加cuda配置（#include “cuda_runtime.h“等飘红问题解决）_vs2019 c++ cuda-CSDN博客</a></p>
<h3 id="Q2-CUDA-开发中的设备与主机"><a href="#Q2-CUDA-开发中的设备与主机" class="headerlink" title="Q2 CUDA 开发中的设备与主机"></a>Q2 CUDA 开发中的设备与主机</h3><p>通常主机指CPU，设备指GPU</p>
<h3 id="Q3-核函数中可以使用C-输入输出流吗（cout、cin）"><a href="#Q3-核函数中可以使用C-输入输出流吗（cout、cin）" class="headerlink" title="Q3 核函数中可以使用C++输入输出流吗（cout、cin）"></a>Q3 核函数中可以使用C++输入输出流吗（cout、cin）</h3><p>核函数中不可以使用<code>cout</code>和<code>cin</code>，但是<code>.cu</code>文件中其他函数（非核函数）可以使用<code>cout</code>和<code>cin</code>。</p>
<h3 id="Q4-CUDA正确运行用户名"><a href="#Q4-CUDA正确运行用户名" class="headerlink" title="Q4 CUDA正确运行用户名"></a>Q4 CUDA正确运行用户名</h3><p>CUDA运行成功需要用户名为英文，不能出现中文路径名。</p>
<h3 id="Q5-VS-CUDA新建项目没有CUDA选项的问题"><a href="#Q5-VS-CUDA新建项目没有CUDA选项的问题" class="headerlink" title="Q5 VS+CUDA新建项目没有CUDA选项的问题"></a>Q5 VS+CUDA新建项目没有CUDA选项的问题</h3><p>一般来说这种情况是由于先装CUDA后装VS导致的，解决办法可以参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39591031/article/details/124462430">VS+CUDA 新建项目里没有CUDA选项（附详细图文步骤）_cuda visual studio integration没有勾选,怎么重新下载-CSDN博客</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/04/19/%E6%99%BA%E8%83%BD%E7%BD%91%E7%90%83%E8%BD%A6%E4%B8%AD%E6%8E%A7%E7%B3%BB%E7%BB%9F/" rel="prev" title="智能网球车中控系统">
      <i class="fa fa-chevron-left"></i> 智能网球车中控系统
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/01/11/2025-01-11-Linux-Socket%E9%80%9A%E4%BF%A1%E6%9C%8D%E5%8A%A1%E7%AB%AF%E8%87%AA%E5%8A%A8%E5%85%B3%E9%97%AD/" rel="next" title="Linux Socket通信服务端自动关闭">
      Linux Socket通信服务端自动关闭 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">CUDA硬件结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">基本架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%84%E9%83%A8%E5%88%86%E5%85%B7%E4%BD%93%E7%BB%93%E6%9E%84"><span class="nav-number">2.2.</span> <span class="nav-text">各部分具体结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HBM"><span class="nav-number">2.2.1.</span> <span class="nav-text">HBM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memory-Cotroller"><span class="nav-number">2.2.2.</span> <span class="nav-text">Memory Cotroller</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPC%E3%80%81TPC%E5%92%8CSM"><span class="nav-number">2.2.3.</span> <span class="nav-text">GPC、TPC和SM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L2-Cache"><span class="nav-number">2.2.4.</span> <span class="nav-text">L2 Cache</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NVLink"><span class="nav-number">2.2.5.</span> <span class="nav-text">NVLink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#High-Speed-Hub"><span class="nav-number">2.2.6.</span> <span class="nav-text">High-Speed Hub</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GigaThread-Engine-MIG-Control"><span class="nav-number">2.2.7.</span> <span class="nav-text">GigaThread Engine MIG Control</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU%E7%9A%84%E5%86%85%E9%83%A8%E5%AD%98%E5%82%A8"><span class="nav-number">2.3.</span> <span class="nav-text">GPU的内部存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">2.3.1.</span> <span class="nav-text">全局内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-L2%E7%BC%93%E5%AD%98"><span class="nav-number">2.3.2.</span> <span class="nav-text">L1&#x2F;L2缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E5%86%85%E5%AD%98"><span class="nav-number">2.3.3.</span> <span class="nav-text">局部内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8"><span class="nav-number">2.3.4.</span> <span class="nav-text">寄存器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98"><span class="nav-number">2.3.5.</span> <span class="nav-text">共享内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98"><span class="nav-number">2.3.6.</span> <span class="nav-text">常量内存</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E5%99%A8%EF%BC%88SM%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">流式处理器（SM）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Warp-Scheduler-%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="nav-number">2.4.1.</span> <span class="nav-text">Warp Scheduler(线程调度器)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dispatch-Unit-%E8%B0%83%E5%BA%A6%E5%8D%95%E5%85%83"><span class="nav-number">2.4.2.</span> <span class="nav-text">Dispatch Unit(调度单元)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LD-ST-%E5%AD%98%E5%82%A8%E9%98%9F%E5%88%97"><span class="nav-number">2.4.3.</span> <span class="nav-text">LD&#x2F;ST(存储队列)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SFU-%E7%89%B9%E6%AE%8A%E8%AE%A1%E7%AE%97%E5%8D%95%E5%85%83"><span class="nav-number">2.4.4.</span> <span class="nav-text">SFU(特殊计算单元)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1%E6%95%B0%E6%8D%AE%E7%BC%93%E5%AD%98"><span class="nav-number">2.4.5.</span> <span class="nav-text">L1数据缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tex"><span class="nav-number">2.4.6.</span> <span class="nav-text">Tex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.4.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E8%BD%AF%E4%BB%B6%E7%BC%96%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">CUDA软件编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E6%9E%84%E8%AE%A1%E7%AE%97"><span class="nav-number">3.1.</span> <span class="nav-text">异构计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%9E%84"><span class="nav-number">3.1.1.</span> <span class="nav-text">异构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU-GPU%E5%BC%82%E6%9E%84%E6%9E%B6%E6%9E%84"><span class="nav-number">3.1.2.</span> <span class="nav-text">CPU+GPU异构架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU%E8%AE%A1%E7%AE%97%E6%8C%87%E6%A0%87"><span class="nav-number">3.1.3.</span> <span class="nav-text">GPU计算指标</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%B9%E9%87%8F%E7%89%B9%E5%BE%81"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">容量特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">性能指标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU%E5%92%8CGPU%E7%BA%BF%E7%A8%8B%E5%8C%BA%E5%88%AB"><span class="nav-number">3.1.4.</span> <span class="nav-text">CPU和GPU线程区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="nav-number">3.2.</span> <span class="nav-text">CUDA基本介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-number">3.2.1.</span> <span class="nav-text">执行流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA%E7%9A%84API%E6%8E%A5%E5%8F%A3"><span class="nav-number">3.2.2.</span> <span class="nav-text">CUDA的API接口</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F%E6%B5%81%E7%A8%8B"><span class="nav-number">3.2.3.</span> <span class="nav-text">编写程序流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VS-CUDA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="nav-number">3.2.4.</span> <span class="nav-text">VS CUDA环境配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E2%80%94%E2%80%94hello-world"><span class="nav-number">3.2.5.</span> <span class="nav-text">示例——hello_world</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.3.</span> <span class="nav-text">CUDA编程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%89%8D%E7%BC%80"><span class="nav-number">3.3.1.</span> <span class="nav-text">CUDA编程中的前缀</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">3.3.2.</span> <span class="nav-text">内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cuda%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86API"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">cuda内存管理API</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E7%AE%A1%E7%90%86"><span class="nav-number">3.3.3.</span> <span class="nav-text">线程管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%A0%87%E8%AE%B0"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">线程标记</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">3.3.4.</span> <span class="nav-text">核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E7%BC%96%E5%86%99%E9%99%90%E5%88%B6"><span class="nav-number">3.3.4.1.</span> <span class="nav-text">核函数编写限制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="nav-number">3.3.4.2.</span> <span class="nav-text">核函数开发流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6"><span class="nav-number">3.3.5.</span> <span class="nav-text">核函数计时</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CPU%E8%AE%A1%E6%97%B6"><span class="nav-number">3.3.5.1.</span> <span class="nav-text">CPU计时</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPU%E8%AE%A1%E6%97%B6"><span class="nav-number">3.3.5.2.</span> <span class="nav-text">GPU计时</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B"><span class="nav-number">3.3.6.</span> <span class="nav-text">组织并行线程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%97%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9F%A9%E9%98%B5%E7%B4%A2%E5%BC%95"><span class="nav-number">3.3.6.1.</span> <span class="nav-text">使用块和线程建立矩阵索引</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8cuda%E5%AE%9E%E7%8E%B0%E4%BA%8C%E7%BB%B4%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95"><span class="nav-number">3.3.6.2.</span> <span class="nav-text">使用cuda实现二维矩阵加法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.4.</span> <span class="nav-text">CUDA执行模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">3.4.1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SM"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">SM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F"><span class="nav-number">3.4.1.2.</span> <span class="nav-text">线程束</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SIMD-vs-SIMT"><span class="nav-number">3.4.1.3.</span> <span class="nav-text">SIMD vs SIMT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="nav-number">3.4.1.4.</span> <span class="nav-text">线程束调度器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyper-Q%E6%8A%80%E6%9C%AF"><span class="nav-number">3.4.1.5.</span> <span class="nav-text">Hyper-Q技术</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Profile%E8%BF%9B%E8%A1%8C%E4%BC%98%E5%8C%96"><span class="nav-number">3.4.1.6.</span> <span class="nav-text">使用Profile进行优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-number">3.4.2.</span> <span class="nav-text">线程束执行的本质</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%9D%97"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">线程束和线程块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E5%88%86%E5%8C%96"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">线程束分化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D"><span class="nav-number">3.4.2.3.</span> <span class="nav-text">资源分配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BB%B6%E8%BF%9F%E9%9A%90%E8%97%8F"><span class="nav-number">3.4.2.4.</span> <span class="nav-text">延迟隐藏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%A0%E7%94%A8%E7%8E%87"><span class="nav-number">3.4.2.5.</span> <span class="nav-text">占用率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5"><span class="nav-number">3.4.2.6.</span> <span class="nav-text">同步</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7%E5%88%86%E6%9E%90%E6%A0%B8%E5%87%BD%E6%95%B0%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87"><span class="nav-number">3.4.3.</span> <span class="nav-text">使用工具分析核函数执行效率</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Nsight-Systems-vs-Nsight-Compute"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">Nsight Systems vs Nsight Compute</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Nsight-Compute"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">Nsight Compute</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%8C%96"><span class="nav-number">3.4.4.</span> <span class="nav-text">避免分支化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E4%BC%98%E5%8C%96"><span class="nav-number">3.4.4.1.</span> <span class="nav-text">普通优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#for%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96"><span class="nav-number">3.4.4.2.</span> <span class="nav-text">for循环优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%9C%80%E5%90%8E32%E4%B8%AA%E7%BA%BF%E7%A8%8B%E4%BC%98%E5%8C%96"><span class="nav-number">3.4.4.3.</span> <span class="nav-text">线程束最后32个线程优化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.5.</span> <span class="nav-text">CUDA内存模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="nav-number">3.5.1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84%E7%89%B9%E7%82%B9"><span class="nav-number">3.5.1.1.</span> <span class="nav-text">内存层次结构特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CUDA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B-1"><span class="nav-number">3.5.1.2.</span> <span class="nav-text">CUDA内存模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8-1"><span class="nav-number">3.5.1.3.</span> <span class="nav-text">寄存器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E5%86%85%E5%AD%98"><span class="nav-number">3.5.1.4.</span> <span class="nav-text">本地内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-1"><span class="nav-number">3.5.1.5.</span> <span class="nav-text">共享内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98-1"><span class="nav-number">3.5.1.6.</span> <span class="nav-text">常量内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98"><span class="nav-number">3.5.1.7.</span> <span class="nav-text">纹理内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98-1"><span class="nav-number">3.5.1.8.</span> <span class="nav-text">全局内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPU%E7%BC%93%E5%AD%98"><span class="nav-number">3.5.1.9.</span> <span class="nav-text">GPU缓存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9D%99%E6%80%81%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">3.5.1.10.</span> <span class="nav-text">静态全局内存</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-1"><span class="nav-number">3.5.2.</span> <span class="nav-text">内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E9%87%8A%E6%94%BE"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">内存分配和释放</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E4%BC%A0%E8%BE%93"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">内存传输</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BA%E5%AE%9A%E5%86%85%E5%AD%98"><span class="nav-number">3.5.2.3.</span> <span class="nav-text">固定内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%86%85%E5%AD%98"><span class="nav-number">3.5.2.4.</span> <span class="nav-text">零拷贝内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E8%99%9A%E6%8B%9F%E5%AF%BB%E5%9D%80%EF%BC%88UVA%EF%BC%89"><span class="nav-number">3.5.2.5.</span> <span class="nav-text">统一虚拟寻址（UVA）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80"><span class="nav-number">3.5.2.6.</span> <span class="nav-text">统一内存寻址</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">3.5.3.</span> <span class="nav-text">内存访问模式——全局内存</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E9%BD%90%E5%92%8C%E5%90%88%E5%B9%B6%E8%AE%BF%E9%97%AE"><span class="nav-number">3.5.3.1.</span> <span class="nav-text">对齐和合并访问</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AF%BB%E5%8F%96"><span class="nav-number">3.5.3.2.</span> <span class="nav-text">全局内存读取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E5%86%99%E5%85%A5"><span class="nav-number">3.5.3.3.</span> <span class="nav-text">全局内存写入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E4%BD%93%E6%95%B0%E7%BB%84%E4%B8%8E%E6%95%B0%E7%BB%84%E7%BB%93%E6%9E%84%E4%BD%93"><span class="nav-number">3.5.3.4.</span> <span class="nav-text">结构体数组与数组结构体</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-2"><span class="nav-number">3.5.4.</span> <span class="nav-text">共享内存</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="nav-number">3.5.4.1.</span> <span class="nav-text">内存分配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F"><span class="nav-number">3.5.4.2.</span> <span class="nav-text">访问模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98"><span class="nav-number">3.5.4.3.</span> <span class="nav-text">配置共享内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5-1"><span class="nav-number">3.5.4.4.</span> <span class="nav-text">同步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%8C%E4%B8%BB%E5%BA%8F%E5%92%8C%E5%88%97%E4%B8%BB%E5%BA%8F"><span class="nav-number">3.5.4.5.</span> <span class="nav-text">行主序和列主序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98-2"><span class="nav-number">3.5.5.</span> <span class="nav-text">常量内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AA%E8%AF%BB%E7%BC%93%E5%AD%98"><span class="nav-number">3.5.6.</span> <span class="nav-text">只读缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4"><span class="nav-number">3.5.7.</span> <span class="nav-text">线程束洗牌指令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="nav-number">3.5.7.1.</span> <span class="nav-text">基本介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%9F%E5%86%85%E7%BA%BF%E7%A8%8B%EF%BC%88Lane%EF%BC%89"><span class="nav-number">3.5.7.2.</span> <span class="nav-text">束内线程（Lane）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4%E7%9A%84%E4%B8%8D%E5%90%8C%E5%BD%A2%E5%BC%8F"><span class="nav-number">3.5.7.3.</span> <span class="nav-text">线程束洗牌指令的不同形式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%92%8C%E5%B9%B6%E5%8F%91"><span class="nav-number">3.6.</span> <span class="nav-text">流和并发</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E5%8F%91%E5%92%8C%E5%B9%B6%E8%A1%8C"><span class="nav-number">3.6.1.</span> <span class="nav-text">并发和并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5"><span class="nav-number">3.6.2.</span> <span class="nav-text">同步和异步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%BB%E5%A1%9E%E5%92%8C%E9%9D%9E%E9%98%BB%E5%A1%9E"><span class="nav-number">3.6.3.</span> <span class="nav-text">阻塞和非阻塞</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA-%E6%B5%81%EF%BC%88Stream%EF%BC%89"><span class="nav-number">3.6.4.</span> <span class="nav-text">CUDA 流（Stream）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%BB%98%E8%AE%A4%E6%B5%81"><span class="nav-number">3.6.4.1.</span> <span class="nav-text">默认流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9D%9E%E9%BB%98%E8%AE%A4%E6%B5%81"><span class="nav-number">3.6.4.2.</span> <span class="nav-text">非默认流</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E8%B0%83%E5%BA%A6"><span class="nav-number">3.6.5.</span> <span class="nav-text">流调度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%99%9A%E5%81%87%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">3.6.5.1.</span> <span class="nav-text">虚假的依赖关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyper-Q%E6%8A%80%E6%9C%AF-1"><span class="nav-number">3.6.5.2.</span> <span class="nav-text">Hyper-Q技术</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7"><span class="nav-number">3.6.6.</span> <span class="nav-text">流的优先级</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%9C%89%E6%8C%87%E5%AE%9A%E4%BC%98%E5%85%88%E7%BA%A7%E7%9A%84%E6%B5%81"><span class="nav-number">3.6.6.1.</span> <span class="nav-text">创建有指定优先级的流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9F%A5%E8%AF%A2%E5%BD%93%E5%89%8D%E8%AE%BE%E5%A4%87%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5"><span class="nav-number">3.6.6.2.</span> <span class="nav-text">查询当前设备的优先级分布情况</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA-%E4%BA%8B%E4%BB%B6%EF%BC%88Event%EF%BC%89"><span class="nav-number">3.6.7.</span> <span class="nav-text">CUDA 事件（Event）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E5%90%8C%E6%AD%A5"><span class="nav-number">3.6.8.</span> <span class="nav-text">流同步</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%9D%9E%E9%98%BB%E5%A1%9E%E6%B5%81"><span class="nav-number">3.6.8.1.</span> <span class="nav-text">创建非阻塞流</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E5%BC%8F%E5%90%8C%E6%AD%A5"><span class="nav-number">3.6.8.2.</span> <span class="nav-text">隐式同步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%98%BE%E5%BC%8F%E5%90%8C%E6%AD%A5"><span class="nav-number">3.6.8.3.</span> <span class="nav-text">显式同步</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E9%85%8D%E7%BD%AE%E4%BA%8B%E4%BB%B6"><span class="nav-number">3.6.9.</span> <span class="nav-text">可配置事件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%B5%81%E9%97%B4%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">3.6.10.</span> <span class="nav-text">创建流间依赖关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E5%9B%9E%E8%B0%83"><span class="nav-number">3.6.11.</span> <span class="nav-text">流回调</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">CUDA常见函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cudasetdevice-n"><span class="nav-number">4.1.</span> <span class="nav-text">cudasetdevice(n)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaDeviceReset"><span class="nav-number">4.2.</span> <span class="nav-text">cudaDeviceReset()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaDeviceSynchronize"><span class="nav-number">4.3.</span> <span class="nav-text">cudaDeviceSynchronize()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaMalloc-void-devPtr-size-t-nByte"><span class="nav-number">4.4.</span> <span class="nav-text">cudaMalloc(void **devPtr, size_t nByte)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaMemcpy-d-a-h-a-nByte-cudaMemcpyHostToDevice"><span class="nav-number">4.5.</span> <span class="nav-text">cudaMemcpy(d_a, h_a, nByte, cudaMemcpyHostToDevice)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaMemcpyAsync-void-dst-const-void-src-size-t-count-cudaMemcpyKind-kind-cudaStream-t-stream-0"><span class="nav-number">4.6.</span> <span class="nav-text">cudaMemcpyAsync(void* dst, const void* src, size_t count,cudaMemcpyKind kind, cudaStream_t stream &#x3D; 0)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaMemcpyToSymbol-const-void-symbol-const-void-src-size-t-count-size-t-offset-cudaMemcpyKind-kind"><span class="nav-number">4.7.</span> <span class="nav-text">cudaMemcpyToSymbol(const void *symbol, const void * src,  size_t count, size_t offset, cudaMemcpyKind kind)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaFuncSetCacheConfig-const-void-func-enum-cudaFuncCache"><span class="nav-number">4.8.</span> <span class="nav-text">cudaFuncSetCacheConfig(const void * func,enum cudaFuncCache)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaMemcpyToSymbol-const-void-symbol-const-void-src-size-t-count"><span class="nav-number">4.9.</span> <span class="nav-text">cudaMemcpyToSymbol(const void* symbol,const void *src,size_t count)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaMemcpyFromSymbol-const-void-symbol-const-void-src-size-t-count"><span class="nav-number">4.10.</span> <span class="nav-text">cudaMemcpyFromSymbol(const void* symbol,const void *src,size_t count)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaGetSymbolAddress-void-dptr-devData"><span class="nav-number">4.11.</span> <span class="nav-text">cudaGetSymbolAddress((void**)&amp;dptr,devData)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cudaError-t-cudaMemset-void-devPtr-int-value-size-t-count"><span class="nav-number">4.12.</span> <span class="nav-text">cudaError_t cudaMemset(void *devPtr,int value, size_t count)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E5%B7%A5%E5%85%B7"><span class="nav-number">5.</span> <span class="nav-text">CUDA工具</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Nsight"><span class="nav-number">5.1.</span> <span class="nav-text">Nsight</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Nsight-Compute-1"><span class="nav-number">5.1.1.</span> <span class="nav-text">Nsight Compute</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GUI%E6%A8%A1%E5%BC%8F"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">GUI模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CLI%E6%A8%A1%E5%BC%8F"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">CLI模式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Nsight-System"><span class="nav-number">5.1.2.</span> <span class="nav-text">Nsight System</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GUI%E6%A8%A1%E5%BC%8F-1"><span class="nav-number">5.1.2.1.</span> <span class="nav-text">GUI模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CLI%E6%A8%A1%E5%BC%8F-1"><span class="nav-number">5.1.2.2.</span> <span class="nav-text">CLI模式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E9%93%BE%E6%8E%A5"><span class="nav-number">5.1.3.</span> <span class="nav-text">相关链接</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NVIDIA-Compute-Sanitizer"><span class="nav-number">5.2.</span> <span class="nav-text">NVIDIA Compute Sanitizer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">6.</span> <span class="nav-text">问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Q1-CUDA%E7%BC%96%E7%A8%8B%E4%B8%AD%E5%BC%95%E5%85%A5%E5%A4%B4%E6%96%87%E4%BB%B6%E5%92%8C%E9%93%BE%E6%8E%A5%E5%99%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">6.0.1.</span> <span class="nav-text">Q1 CUDA编程中引入头文件和链接器的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q2-CUDA-%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E8%AE%BE%E5%A4%87%E4%B8%8E%E4%B8%BB%E6%9C%BA"><span class="nav-number">6.0.2.</span> <span class="nav-text">Q2 CUDA 开发中的设备与主机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q3-%E6%A0%B8%E5%87%BD%E6%95%B0%E4%B8%AD%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8C-%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%B5%81%E5%90%97%EF%BC%88cout%E3%80%81cin%EF%BC%89"><span class="nav-number">6.0.3.</span> <span class="nav-text">Q3 核函数中可以使用C++输入输出流吗（cout、cin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q4-CUDA%E6%AD%A3%E7%A1%AE%E8%BF%90%E8%A1%8C%E7%94%A8%E6%88%B7%E5%90%8D"><span class="nav-number">6.0.4.</span> <span class="nav-text">Q4 CUDA正确运行用户名</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q5-VS-CUDA%E6%96%B0%E5%BB%BA%E9%A1%B9%E7%9B%AE%E6%B2%A1%E6%9C%89CUDA%E9%80%89%E9%A1%B9%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">6.0.5.</span> <span class="nav-text">Q5 VS+CUDA新建项目没有CUDA选项的问题</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Cedric Chen"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">Cedric Chen</p>
  <div class="site-description" itemprop="description">对着生活哈哈大笑</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ArdrewChen" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ArdrewChen" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cedric Chen</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/xxw/anime.min.js"></script>
  <script src="/xxw/velocity/velocity.min.js"></script>
  <script src="/xxw/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
